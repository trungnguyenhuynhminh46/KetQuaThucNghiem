{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0627faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85423270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phần code này dựa trên bài viết này\n",
    "# https://www.sciencedirect.com/science/article/pii/S1877050919302789"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab6eea",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7cfe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout, LeakyReLU, Flatten\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "from pandas_datareader import data as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ea8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm vẽ mô hình miêu tả giá trị từng thuộc tính\n",
    "def plot_dataframe(df):\n",
    "  for column_name in df.columns:\n",
    "    plt.figure()\n",
    "    plt.title(column_name)\n",
    "    plt.plot(df[column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64236b5a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94817d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm thêm cột giá trị Ma, là đường trung bình động (Moving Average) của biến giá trị Close\n",
    "def add_Ma(df, window=5):\n",
    "    for i in range(window, df.shape[0]):\n",
    "        sum = 0.0\n",
    "        for k in range(1, window+1):\n",
    "            sum += df.iloc[i-k, 4]\n",
    "        df.loc[df.index[i], 'Ma'] = np.round(sum/window, 6)\n",
    "    return df[window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68bdddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class này kế thừa class TimeseriesGenerator\n",
    "# Nó tạo ra các batch dữ liệu được chuẩn hóa (standarized)\n",
    "class Standarized_TimeseriesGenerator(tf.keras.preprocessing.sequence.TimeseriesGenerator):\n",
    "  def __getitem__(self, index):\n",
    "    samples, targets  = super(Standarized_TimeseriesGenerator, self).__getitem__(index)\n",
    "    mean = samples.mean(axis=1)\n",
    "    std = samples.std(axis=1)\n",
    "    samples = (samples - mean[:,None,:])/std[:,None,:]\n",
    "    targets = (targets - mean)/std\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20efe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giải thích các thông số đầu vào của class TimeseriesGenerator\n",
    "# Data: Những dữ liệu liên tiếp nhau (axis 0 được kì vọng là time dimension)\n",
    "# Target: Dữ liệu tương ứng với những timesteps trong data\n",
    "# Length: Độ dài của các sequences đầu vào (Hay window, look-back)\n",
    "# sampling_rate: Chu kỳ của các timestep input được lấy (VD: sampling_rate=2 \n",
    "# thì cách 2 ngày lấy một lần)\n",
    "# stride: Chu kỳ của các timestep output được lấy (VD: stride=2 \n",
    "# thì cách 2 ngày lấy một lần, hay chú Sơn nói là số bước mà cửa sổ trượt)\n",
    "# start_index, end_index: Khoảng dữ liệu được dùng\n",
    "# bacth_size: số lượng samples 1 batch\n",
    "# ([day1, day2, day3],[day4]) là 1 sample\n",
    "# [([day1, day2, day3],[day4]), ([day2, day3, day4],[day5])] là 1 batch có \n",
    "# batch_size là 5\n",
    "################Xét ví dụ sau############\n",
    "# from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "# import numpy as np\n",
    "# data = np.array([[i] for i in range(50)])\n",
    "# targets = np.array([[i] for i in range(50)])\n",
    "# data_gen = TimeseriesGenerator(data, targets,\n",
    "#                                length=10, sampling_rate=2,\n",
    "#                                batch_size=2)\n",
    "# assert len(data_gen) == 20\n",
    "# batch_0 = data_gen[0]\n",
    "# x, y = batch_0\n",
    "# assert np.array_equal(x,\n",
    "#                       np.array([[[0], [2], [4], [6], [8]],\n",
    "#                                 [[1], [3], [5], [7], [9]]]))\n",
    "# assert np.array_equal(y,\n",
    "#                       np.array([[10], [11]]))\n",
    "# ==>length=10 nên những dữ liệu được pick cho input có index 0->9\n",
    "# Và dữ liệu đầu tiên cho output có index là 10\n",
    "# sampling_rate bằng 2 nên (index thứ i) - (index thứ i - 1) = 2\n",
    "# Batch_size=2 --> Mỗi batch có 2 sample mà có 41 samples --> Có 21 batches\n",
    "\n",
    "def get_gen_train_test(dataframe, n_sequence, n_batch):\n",
    "    data = dataframe.drop(columns='Date').to_numpy()\n",
    "    targets = data\n",
    "    n_samples = data.shape[0]\n",
    "    train_test_split=int(n_samples*0.9)\n",
    "\n",
    "    data_gen_train = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = 0,\n",
    "                                end_index = train_test_split,\n",
    "                                shuffle = True)\n",
    "    data_gen_test = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = train_test_split,\n",
    "                                end_index = n_samples-1)\n",
    "\n",
    "    return data_gen_train, data_gen_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596c86e",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5d3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm dùng để tính mean squared error cho generated data\n",
    "def mean_squared_error(dataset):\n",
    "    mse=0\n",
    "    for X_batch, y_batch in dataset:\n",
    "        mse += np.mean(np.square(X_batch[:, -1, 3:4]-y_batch[:, 3:4]))\n",
    "    mse /= len(dataset)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82bae3",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63452d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các hàm tính metrics từ 2 tập thực tế và dự đoán.\n",
    "def mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3]))\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def mape(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])/y_true[:,3]))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3])))\n",
    "def ar(y_true, y_pred):\n",
    "    mask = tf.cast(y_pred[1:,3] > y_true[:-1,3],tf.float32)\n",
    "    return tf.reduce_mean((y_true[1:,3]-y_true[:-1,3])*mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d7d3",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0763cd9",
   "metadata": {},
   "source": [
    "## Perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e3849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vì Discriminator trong bài toán này chỉ có hai giá trị đầu ra (giống/ không giống) nên dùng hàm phân loại nhị phân\n",
    "# Tìm hiểu Binary Cross Entropy là gì tại đây:\n",
    "# https://ndquy.github.io/posts/loss-function-p2/#:~:text=Binary%20Cross%2DEntropy%20Loss,tr%E1%BB%8B%20%7B0%2C%201%7D.\n",
    "# Cross-entropy là hàm loss được sử dụng mặc định cho bài toán phân lớp nhị phân.\n",
    "# Nó được thiết kế để sử dụng với bài toán phân loại nhị phân trong đó các giá trị mục tiêu nhận một trong 2 giá trị {0, 1}.\n",
    "# Trang 402\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    # So sách kết quả dự đoán như thể nào với 1 (đúng) (real_loss)\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    # So sách kết quả dự đoán như thể nào với 1 (đúng) (real_loss)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    # Giá trị mất mát của Generator\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "# https://www.sciencedirect.com/science/article/pii/S1877050919302789\n",
    "# Trang 403\n",
    "def generator_loss(x, y, fake_output):\n",
    "    a1=0.01\n",
    "    g_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_mse = tf.keras.losses.MSE(x, y)\n",
    "    return a1*g_mse + (1-a1)*g_loss, g_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda87fb",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a10957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(n_sequence, n_features):\n",
    "    inputs = Input(shape=(n_sequence, n_features,))\n",
    "    # return_sequences = True, Trả về cho lớp sau các điểm đầu ra trước đó thay vì chỉ các điểm đầu ra ở lớp cuối cùng\n",
    "    # Trong mạng neural học sâu, kernel_initializer là một tham số được sử dụng để khởi tạo trọng số (weights) của các lớp (layer) trong mạng neural.\n",
    "    lstm_1 = LSTM(units=10, return_sequences = True, activation=None, kernel_initializer='random_normal')(inputs)\n",
    "    # Batch Normalization là một lớp (layer) được sử dụng trong mạng neural học sâu để chuẩn hóa đầu ra của một lớp trước khi đưa vào lớp kế tiếp\n",
    "    # Cải thiện \"vanishing gradient\" và \"exploding gradient\", đồng thời giúp mô hình hội tụ nhanh hơn\n",
    "    batch_norm1=tf.keras.layers.BatchNormalization()(lstm_1)\n",
    "    lstm_1_LRelu = LeakyReLU(alpha=0.3)(batch_norm1) \n",
    "    lstm_1_droput = Dropout(0.3)(lstm_1_LRelu)\n",
    "    lstm_2 = LSTM(units=10, return_sequences = False, activation=None, kernel_initializer='random_normal')(lstm_1_droput)\n",
    "    batch_norm2=tf.keras.layers.BatchNormalization()(lstm_2)\n",
    "    lstm_2_LRelu = LeakyReLU(alpha=0.3)(batch_norm2) \n",
    "    lstm_2_droput = Dropout(0.3)(lstm_2_LRelu)\n",
    "    output_dense = Dense(n_features, activation=None)(lstm_2_droput)\n",
    "    #  LeakyReLU\n",
    "    # https://aicurious.io/blog/2019-09-23-cac-ham-kich-hoat-activation-function-trong-neural-networks\n",
    "    output = LeakyReLU(alpha=0.3)(output_dense) \n",
    "\n",
    "    model = Model(inputs = inputs, outputs = output)\n",
    "    model.compile(loss=None, metrics = [mse , mae, mape, rmse, ar])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "# Vì sao cần hàm kích hoạt, vì nếu có hàm kích hoạt mà là các lớp nơ ron đơn thuần xếp chồng lên nhau thì kết quả đầu ra đơn \n",
    "# là một hàm tuyến tính của giá trị đầu vào --> Việc xếp chồng mà k có hàm kích hoạt là vô nghĩa\n",
    "# Sigmoid (0,1). Đạo hàm đẹp nhưng dễ bằng 0 (vanishing gradient), không có trung tâm nên hội tụ khó khăn\n",
    "# tanh (-1,1). Cũng bão hòa hai đầu nhưng có trung tâm (O)\n",
    "# ReLu max(0, x): Sử dụng khá nhiều gần đây do tốc độ hội tụ nhanh do nó k bị bão hòa 2 đầu\n",
    "# --> Rectified Linear Unit: đơn vị tuyến tính được hiệu chỉnh\n",
    "# Tính toán nhanh dO công thức đơn giản\n",
    "# Nhược điểm là với những node < 0 thù activation chuyển thành 0 (Dying ReLU) (*)\n",
    "# Leaky ReLU dùng để khắc phục ReLU\n",
    "# LeakyReLU: f(x) = 1 với x<0 và f(x)=ax + 1 với x>=0 với a nhỏ\n",
    "# Thay vì trả về 0 với x < 0 thì nó cố gắng tạo ra một đường xiên nhỏ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73434c04",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbabdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=72, input_shape=((n_sequence+1) * n_features,), activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    # là một lớp sinh dữ liệu ngẫu nhiên nhằm giảm thiểu overfit\n",
    "    model.add(tf.keras.layers.GaussianNoise(stddev=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=100, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=10, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1 ,activation='sigmoid'))\n",
    "    model.compile(loss=discriminator_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29f52d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5acba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_def(sequences, sequences_end):\n",
    "    # GradientTape trả về  Context Manager, quản lý quan sát các thay đổi của mô hình và cập nhật lại trọng số\n",
    "    # Nó theo dõi các quá trình tính toán của mô hình trong quá trình lan truyền thuận và cập nhật trong quá trình lan truyền ngược\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Giải thích training=True \n",
    "        # https://stackoverflow.com/questions/57320371/what-does-training-true-mean-when-calling-a-tensorflow-keras-model\n",
    "        generated_prediction = generator(sequences, training=True)\n",
    "        # None là cách tăng dimension. VD: [1,2,3] --> [[1],[2],[3]]\n",
    "        sequences_true = tf.concat((sequences, sequences_end[:, None, :]), axis=1)\n",
    "        sequences_fake = tf.concat((sequences, generated_prediction[:, None, :]), axis=1)\n",
    "\n",
    "        real_output = discriminator(sequences_true, training=True)\n",
    "        fake_output = discriminator(sequences_fake, training=True)\n",
    "\n",
    "        gen_loss, gen_mse_loss = generator_loss(generated_prediction, \n",
    "                                                sequences_end, \n",
    "                                                fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)\n",
    "\n",
    "def test_step_def(sequences, sequences_end):\n",
    "    generated_prediction = generator(sequences, training=False)\n",
    "\n",
    "    sequences_true = tf.concat((sequences, sequences_end[:,None,:]), axis=1)\n",
    "    sequences_fake = tf.concat((sequences, generated_prediction[:,None,:]), axis=1)\n",
    "\n",
    "    real_output = discriminator(sequences_true, training=False)\n",
    "    fake_output = discriminator(sequences_fake, training=False)\n",
    "\n",
    "    gen_loss, gen_mse_loss = generator_loss(generated_prediction, sequences_end, fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81363ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, dataset_val, epochs):\n",
    "    # Biến history (training) và history_val (testing) lưu các thông số liên quan đến mse, rmse, mae, mape để vẽ đồ thị ở bước sau\n",
    "    history = np.empty(shape = (8, epochs))\n",
    "    history_val = np.empty(shape = (8, epochs))\n",
    "    len_dataset = len(dataset)\n",
    "    len_dataset_val = len(dataset_val)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        cur_dis_loss = 0\n",
    "        cur_gen_loss = 0\n",
    "        cur_gen_mse_loss = 0\n",
    "        # Với mỗi batch\n",
    "        for sequence_batch, sequence_end_batch in dataset:\n",
    "            # Tiến hành train, cập nhật weight bằng Gradient Tape cho hai mô hình G và D\n",
    "            aux_cur_losses = train_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                      tf.cast(sequence_end_batch, tf.float32))\n",
    "            # lấy ra giá trị của các loss function \n",
    "            cur_gen_loss += aux_cur_losses[0]/len_dataset\n",
    "            cur_dis_loss += aux_cur_losses[1]/len_dataset\n",
    "            cur_gen_mse_loss += aux_cur_losses[2]/len_dataset\n",
    "        # Verbose là dài dòng, verbose=False có nghĩa là không xuất ra giá trị cụ thể\n",
    "        cur_gen_metrics = generator.evaluate(dataset,verbose=False)[1:]\n",
    "\n",
    "        #Lưu các thông số vào biến history\n",
    "        history[:, epoch] = cur_gen_loss, cur_dis_loss, cur_gen_mse_loss, *cur_gen_metrics\n",
    "\n",
    "        cur_gen_metrics_val = generator.evaluate(dataset_val,verbose=False)[1: ]\n",
    "\n",
    "        cur_gen_loss_val = 0\n",
    "        cur_dis_loss_val = 0\n",
    "        cur_gen_mse_loss_val = 0\n",
    "        for sequence_batch, sequence_end_batch in dataset_val:\n",
    "            aux_cur_losses_val = test_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                         tf.cast(sequence_end_batch, tf.float32))\n",
    "            cur_gen_loss_val += aux_cur_losses_val[0]/len_dataset_val\n",
    "            cur_dis_loss_val += aux_cur_losses_val[1]/len_dataset_val\n",
    "            cur_gen_mse_loss_val += aux_cur_losses_val[2]/len_dataset_val\n",
    "    \n",
    "\n",
    "\n",
    "        history_val[:, epoch] = cur_gen_loss_val, cur_dis_loss_val, cur_gen_mse_loss_val, *cur_gen_metrics_val\n",
    "\n",
    "        print ('Time for epoch {} is {} sec Generator Loss: {},  Discriminator_loss: {}'\n",
    "               .format(epoch + 1, time.time()-start, cur_gen_loss, cur_dis_loss))\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    return history, history_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca9168",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5119da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history_val):\n",
    "    metrics = [\"gen_loss\",\"dis_loss\",\"gen_mse_loss\", 'mse','mae','mape','rmse','ar']\n",
    "    for i, metric_name in enumerate(metrics):  \n",
    "        plt.figure()\n",
    "        plt.title(metric_name)\n",
    "        plt.plot(history[i], label='train')\n",
    "        plt.plot(history_val[i], label='test')\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aae39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(sequence, target, model):\n",
    "    y_pred = model.predict(sequence)[...,3]\n",
    "    y_true = target[...,3]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"closing price\")\n",
    "    plt.plot(y_true, label=\"true\")\n",
    "    plt.plot(y_pred, label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6a41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results(history):\n",
    "    min_index = np.argmin(history[3, :])\n",
    "    return history[:, min_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc15b2",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef9e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "n_sequence = window\n",
    "n_features = 7\n",
    "n_batch = 50\n",
    "EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb9e4a",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13605d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-07-07</td>\n",
       "      <td>1.093333</td>\n",
       "      <td>1.108667</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>103825500</td>\n",
       "      <td>1.399867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-07-08</td>\n",
       "      <td>1.076000</td>\n",
       "      <td>1.168000</td>\n",
       "      <td>1.038000</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>115671000</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-07-09</td>\n",
       "      <td>1.172000</td>\n",
       "      <td>1.193333</td>\n",
       "      <td>1.103333</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>60759000</td>\n",
       "      <td>1.207067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-07-12</td>\n",
       "      <td>1.196667</td>\n",
       "      <td>1.204667</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>1.136667</td>\n",
       "      <td>1.136667</td>\n",
       "      <td>33037500</td>\n",
       "      <td>1.146267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-07-13</td>\n",
       "      <td>1.159333</td>\n",
       "      <td>1.242667</td>\n",
       "      <td>1.126667</td>\n",
       "      <td>1.209333</td>\n",
       "      <td>1.209333</td>\n",
       "      <td>40201500</td>\n",
       "      <td>1.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>210.733337</td>\n",
       "      <td>217.166672</td>\n",
       "      <td>207.523331</td>\n",
       "      <td>215.326660</td>\n",
       "      <td>215.326660</td>\n",
       "      <td>99519000</td>\n",
       "      <td>217.591333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>214.330002</td>\n",
       "      <td>222.029999</td>\n",
       "      <td>213.666672</td>\n",
       "      <td>220.589996</td>\n",
       "      <td>220.589996</td>\n",
       "      <td>68596800</td>\n",
       "      <td>219.138666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>224.836670</td>\n",
       "      <td>227.133331</td>\n",
       "      <td>220.266663</td>\n",
       "      <td>221.229996</td>\n",
       "      <td>221.229996</td>\n",
       "      <td>96835800</td>\n",
       "      <td>219.529999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>220.333328</td>\n",
       "      <td>223.300003</td>\n",
       "      <td>218.333328</td>\n",
       "      <td>221.996674</td>\n",
       "      <td>221.996674</td>\n",
       "      <td>68732400</td>\n",
       "      <td>217.442664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>232.199997</td>\n",
       "      <td>222.786667</td>\n",
       "      <td>231.593338</td>\n",
       "      <td>231.593338</td>\n",
       "      <td>128538000</td>\n",
       "      <td>218.517999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2641 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "5    2010-07-07    1.093333    1.108667    0.998667    1.053333    1.053333   \n",
       "6    2010-07-08    1.076000    1.168000    1.038000    1.164000    1.164000   \n",
       "7    2010-07-09    1.172000    1.193333    1.103333    1.160000    1.160000   \n",
       "8    2010-07-12    1.196667    1.204667    1.133333    1.136667    1.136667   \n",
       "9    2010-07-13    1.159333    1.242667    1.126667    1.209333    1.209333   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2641 2020-12-23  210.733337  217.166672  207.523331  215.326660  215.326660   \n",
       "2642 2020-12-24  214.330002  222.029999  213.666672  220.589996  220.589996   \n",
       "2643 2020-12-28  224.836670  227.133331  220.266663  221.229996  221.229996   \n",
       "2644 2020-12-29  220.333328  223.300003  218.333328  221.996674  221.996674   \n",
       "2645 2020-12-30  224.000000  232.199997  222.786667  231.593338  231.593338   \n",
       "\n",
       "         Volume          Ma  \n",
       "5     103825500    1.399867  \n",
       "6     115671000    1.292000  \n",
       "7      60759000    1.207067  \n",
       "8      33037500    1.146267  \n",
       "9      40201500    1.117600  \n",
       "...         ...         ...  \n",
       "2641   99519000  217.591333  \n",
       "2642   68596800  219.138666  \n",
       "2643   96835800  219.529999  \n",
       "2644   68732400  217.442664  \n",
       "2645  128538000  218.517999  \n",
       "\n",
       "[2641 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_code = \"TSLA\"\n",
    "start = dt.datetime(2000, 1, 1)\n",
    "end = dt.datetime(2020, 12, 31)\n",
    "raw_data = pdr.get_data_yahoo(stock_code, start, end,threads=False, proxy=\"http://127.0.0.1:7890\")\n",
    "df = raw_data.dropna();\n",
    "df = df.reset_index(level=0)\n",
    "df = add_Ma(df, window)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 5, 7)]            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 5, 10)             720       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 5, 10)            40        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 5, 10)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 5, 10)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 10)                840       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10)               40        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 77        \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,717\n",
      "Trainable params: 1,677\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n",
      "Time for epoch 1 is 8.626598834991455 sec Generator Loss: 0.7508279085159302,  Discriminator_loss: 1.3811664581298828\n",
      "Time for epoch 2 is 0.46454954147338867 sec Generator Loss: 0.7342929840087891,  Discriminator_loss: 1.3626245260238647\n",
      "Time for epoch 3 is 0.46894311904907227 sec Generator Loss: 0.7341001629829407,  Discriminator_loss: 1.3380860090255737\n",
      "Time for epoch 4 is 0.47670722007751465 sec Generator Loss: 0.7383900880813599,  Discriminator_loss: 1.30107581615448\n",
      "Time for epoch 5 is 0.5243372917175293 sec Generator Loss: 0.7577442526817322,  Discriminator_loss: 1.2611912488937378\n",
      "Time for epoch 6 is 0.48715877532958984 sec Generator Loss: 0.7793594598770142,  Discriminator_loss: 1.2069294452667236\n",
      "Time for epoch 7 is 0.4847121238708496 sec Generator Loss: 0.8183836340904236,  Discriminator_loss: 1.1374385356903076\n",
      "Time for epoch 8 is 0.4681229591369629 sec Generator Loss: 0.8569485545158386,  Discriminator_loss: 1.087567925453186\n",
      "Time for epoch 9 is 0.49536752700805664 sec Generator Loss: 0.9272235631942749,  Discriminator_loss: 0.9944162964820862\n",
      "Time for epoch 10 is 0.5297451019287109 sec Generator Loss: 1.013087272644043,  Discriminator_loss: 0.9292449355125427\n",
      "Time for epoch 11 is 0.5460302829742432 sec Generator Loss: 1.0952953100204468,  Discriminator_loss: 0.8767594695091248\n",
      "Time for epoch 12 is 0.46092653274536133 sec Generator Loss: 1.1807360649108887,  Discriminator_loss: 0.8272914290428162\n",
      "Time for epoch 13 is 0.46192216873168945 sec Generator Loss: 1.2415573596954346,  Discriminator_loss: 0.7950905561447144\n",
      "Time for epoch 14 is 0.46799564361572266 sec Generator Loss: 1.3308378458023071,  Discriminator_loss: 0.7488135099411011\n",
      "Time for epoch 15 is 0.538811445236206 sec Generator Loss: 1.3803423643112183,  Discriminator_loss: 0.7557909488677979\n",
      "Time for epoch 16 is 0.5482625961303711 sec Generator Loss: 1.4284902811050415,  Discriminator_loss: 0.7530904412269592\n",
      "Time for epoch 17 is 0.5255172252655029 sec Generator Loss: 1.4734852313995361,  Discriminator_loss: 0.7579116225242615\n",
      "Time for epoch 18 is 0.4977569580078125 sec Generator Loss: 1.475280523300171,  Discriminator_loss: 0.7840970158576965\n",
      "Time for epoch 19 is 0.5696463584899902 sec Generator Loss: 1.5193846225738525,  Discriminator_loss: 0.8047244548797607\n",
      "Time for epoch 20 is 0.4974992275238037 sec Generator Loss: 1.5107412338256836,  Discriminator_loss: 0.8554865717887878\n",
      "Time for epoch 21 is 0.50351881980896 sec Generator Loss: 1.5166912078857422,  Discriminator_loss: 0.8492882251739502\n",
      "Time for epoch 22 is 0.47933340072631836 sec Generator Loss: 1.5187640190124512,  Discriminator_loss: 0.8436470031738281\n",
      "Time for epoch 23 is 0.5195469856262207 sec Generator Loss: 1.5178028345108032,  Discriminator_loss: 0.8623403906822205\n",
      "Time for epoch 24 is 0.5190467834472656 sec Generator Loss: 1.4764254093170166,  Discriminator_loss: 0.8918595910072327\n",
      "Time for epoch 25 is 0.5189836025238037 sec Generator Loss: 1.5151411294937134,  Discriminator_loss: 0.8579046130180359\n",
      "Time for epoch 26 is 0.49698328971862793 sec Generator Loss: 1.5151773691177368,  Discriminator_loss: 0.905296802520752\n",
      "Time for epoch 27 is 0.4920158386230469 sec Generator Loss: 1.461259365081787,  Discriminator_loss: 0.9051984548568726\n",
      "Time for epoch 28 is 0.47876739501953125 sec Generator Loss: 1.4729750156402588,  Discriminator_loss: 0.8926992416381836\n",
      "Time for epoch 29 is 0.4785900115966797 sec Generator Loss: 1.4735592603683472,  Discriminator_loss: 0.9101637601852417\n",
      "Time for epoch 30 is 0.48387789726257324 sec Generator Loss: 1.4668763875961304,  Discriminator_loss: 0.8993392586708069\n",
      "Time for epoch 31 is 0.5382051467895508 sec Generator Loss: 1.4568225145339966,  Discriminator_loss: 0.9407556653022766\n",
      "Time for epoch 32 is 0.5639159679412842 sec Generator Loss: 1.444390892982483,  Discriminator_loss: 0.9152858853340149\n",
      "Time for epoch 33 is 0.49474287033081055 sec Generator Loss: 1.4401109218597412,  Discriminator_loss: 0.9364309906959534\n",
      "Time for epoch 34 is 0.5112285614013672 sec Generator Loss: 1.4447122812271118,  Discriminator_loss: 0.9256495237350464\n",
      "Time for epoch 35 is 0.47124648094177246 sec Generator Loss: 1.4153305292129517,  Discriminator_loss: 0.9179732203483582\n",
      "Time for epoch 36 is 0.492403507232666 sec Generator Loss: 1.475419044494629,  Discriminator_loss: 0.9241344928741455\n",
      "Time for epoch 37 is 0.4653348922729492 sec Generator Loss: 1.3904106616973877,  Discriminator_loss: 0.9229815602302551\n",
      "Time for epoch 38 is 0.4645843505859375 sec Generator Loss: 1.3953306674957275,  Discriminator_loss: 0.9463284015655518\n",
      "Time for epoch 39 is 0.5285236835479736 sec Generator Loss: 1.454000473022461,  Discriminator_loss: 0.9025439620018005\n",
      "Time for epoch 40 is 0.508697509765625 sec Generator Loss: 1.4314004182815552,  Discriminator_loss: 0.9026118516921997\n",
      "Time for epoch 41 is 0.48419904708862305 sec Generator Loss: 1.409169316291809,  Discriminator_loss: 0.9210425615310669\n",
      "Time for epoch 42 is 0.48874926567077637 sec Generator Loss: 1.3996785879135132,  Discriminator_loss: 0.9316885471343994\n",
      "Time for epoch 43 is 0.4611494541168213 sec Generator Loss: 1.4101207256317139,  Discriminator_loss: 0.9185594320297241\n",
      "Time for epoch 44 is 0.5350961685180664 sec Generator Loss: 1.390080451965332,  Discriminator_loss: 0.9335049390792847\n",
      "Time for epoch 45 is 0.4800736904144287 sec Generator Loss: 1.3906067609786987,  Discriminator_loss: 0.9433923959732056\n",
      "Time for epoch 46 is 0.5397076606750488 sec Generator Loss: 1.4188601970672607,  Discriminator_loss: 0.9319729804992676\n",
      "Time for epoch 47 is 0.48041224479675293 sec Generator Loss: 1.4055956602096558,  Discriminator_loss: 0.9190903306007385\n",
      "Time for epoch 48 is 0.483659029006958 sec Generator Loss: 1.3544002771377563,  Discriminator_loss: 0.9744285941123962\n",
      "Time for epoch 49 is 0.47715234756469727 sec Generator Loss: 1.3747137784957886,  Discriminator_loss: 0.9413169026374817\n",
      "Time for epoch 50 is 0.47940850257873535 sec Generator Loss: 1.3796859979629517,  Discriminator_loss: 0.9242741465568542\n",
      "Time for epoch 51 is 0.49269628524780273 sec Generator Loss: 1.3731880187988281,  Discriminator_loss: 0.9439760446548462\n",
      "Time for epoch 52 is 0.5425660610198975 sec Generator Loss: 1.3721542358398438,  Discriminator_loss: 0.9451795220375061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 53 is 0.489255428314209 sec Generator Loss: 1.3574144840240479,  Discriminator_loss: 0.9597996473312378\n",
      "Time for epoch 54 is 0.5049746036529541 sec Generator Loss: 1.3300485610961914,  Discriminator_loss: 0.9645456075668335\n",
      "Time for epoch 55 is 0.4943063259124756 sec Generator Loss: 1.359990119934082,  Discriminator_loss: 0.9325014352798462\n",
      "Time for epoch 56 is 0.5469357967376709 sec Generator Loss: 1.3302241563796997,  Discriminator_loss: 0.9530340433120728\n",
      "Time for epoch 57 is 0.4889042377471924 sec Generator Loss: 1.3388519287109375,  Discriminator_loss: 0.9521411061286926\n",
      "Time for epoch 58 is 0.468503475189209 sec Generator Loss: 1.324339509010315,  Discriminator_loss: 0.9703111052513123\n",
      "Time for epoch 59 is 0.5767877101898193 sec Generator Loss: 1.3376655578613281,  Discriminator_loss: 0.9524899125099182\n",
      "Time for epoch 60 is 0.5110280513763428 sec Generator Loss: 1.3170897960662842,  Discriminator_loss: 0.9701407551765442\n",
      "Time for epoch 61 is 0.46924924850463867 sec Generator Loss: 1.2959004640579224,  Discriminator_loss: 0.9793809056282043\n",
      "Time for epoch 62 is 0.47798633575439453 sec Generator Loss: 1.3321099281311035,  Discriminator_loss: 0.9478947520256042\n",
      "Time for epoch 63 is 0.46733546257019043 sec Generator Loss: 1.2896584272384644,  Discriminator_loss: 0.9941246509552002\n",
      "Time for epoch 64 is 0.482785701751709 sec Generator Loss: 1.290464997291565,  Discriminator_loss: 0.9870632886886597\n",
      "Time for epoch 65 is 0.48899149894714355 sec Generator Loss: 1.2712771892547607,  Discriminator_loss: 1.0013744831085205\n",
      "Time for epoch 66 is 0.47521281242370605 sec Generator Loss: 1.2767906188964844,  Discriminator_loss: 1.0288643836975098\n",
      "Time for epoch 67 is 0.5447673797607422 sec Generator Loss: 1.2675787210464478,  Discriminator_loss: 1.0404397249221802\n",
      "Time for epoch 68 is 0.55289626121521 sec Generator Loss: 1.2449625730514526,  Discriminator_loss: 1.0652061700820923\n",
      "Time for epoch 69 is 0.6123931407928467 sec Generator Loss: 1.2392727136611938,  Discriminator_loss: 1.078739881515503\n",
      "Time for epoch 70 is 0.6100876331329346 sec Generator Loss: 1.2158398628234863,  Discriminator_loss: 1.102839708328247\n",
      "Time for epoch 71 is 0.573549747467041 sec Generator Loss: 1.1831111907958984,  Discriminator_loss: 1.1554288864135742\n",
      "Time for epoch 72 is 0.49073147773742676 sec Generator Loss: 1.1978089809417725,  Discriminator_loss: 1.1471426486968994\n",
      "Time for epoch 73 is 0.5338778495788574 sec Generator Loss: 1.1757190227508545,  Discriminator_loss: 1.1268339157104492\n",
      "Time for epoch 74 is 0.7877950668334961 sec Generator Loss: 1.1669458150863647,  Discriminator_loss: 1.1144238710403442\n",
      "Time for epoch 75 is 0.5836527347564697 sec Generator Loss: 1.1518980264663696,  Discriminator_loss: 1.1282562017440796\n",
      "Time for epoch 76 is 0.5561051368713379 sec Generator Loss: 1.1568167209625244,  Discriminator_loss: 1.1343790292739868\n",
      "Time for epoch 77 is 0.6237118244171143 sec Generator Loss: 1.1496379375457764,  Discriminator_loss: 1.1150776147842407\n",
      "Time for epoch 78 is 0.623812198638916 sec Generator Loss: 1.1605570316314697,  Discriminator_loss: 1.1119409799575806\n",
      "Time for epoch 79 is 0.5951368808746338 sec Generator Loss: 1.1261078119277954,  Discriminator_loss: 1.1335642337799072\n",
      "Time for epoch 80 is 0.5587761402130127 sec Generator Loss: 1.1497191190719604,  Discriminator_loss: 1.11210298538208\n",
      "Time for epoch 81 is 0.5504674911499023 sec Generator Loss: 1.120185136795044,  Discriminator_loss: 1.1504892110824585\n",
      "Time for epoch 82 is 0.5058536529541016 sec Generator Loss: 1.1241295337677002,  Discriminator_loss: 1.1346452236175537\n",
      "Time for epoch 83 is 0.541633129119873 sec Generator Loss: 1.114591360092163,  Discriminator_loss: 1.1316860914230347\n",
      "Time for epoch 84 is 0.536590576171875 sec Generator Loss: 1.1037551164627075,  Discriminator_loss: 1.1395714282989502\n",
      "Time for epoch 85 is 0.48020505905151367 sec Generator Loss: 1.0835272073745728,  Discriminator_loss: 1.164853811264038\n",
      "Time for epoch 86 is 0.5366034507751465 sec Generator Loss: 1.07979154586792,  Discriminator_loss: 1.14640212059021\n",
      "Time for epoch 87 is 0.5003232955932617 sec Generator Loss: 1.0957880020141602,  Discriminator_loss: 1.1689962148666382\n",
      "Time for epoch 88 is 0.46121716499328613 sec Generator Loss: 1.0927512645721436,  Discriminator_loss: 1.1297907829284668\n",
      "Time for epoch 89 is 0.47423672676086426 sec Generator Loss: 1.0766433477401733,  Discriminator_loss: 1.169941782951355\n",
      "Time for epoch 90 is 0.4788787364959717 sec Generator Loss: 1.0833687782287598,  Discriminator_loss: 1.166295051574707\n",
      "Time for epoch 91 is 0.5349016189575195 sec Generator Loss: 1.055763840675354,  Discriminator_loss: 1.1761890649795532\n",
      "Time for epoch 92 is 0.5506303310394287 sec Generator Loss: 1.0510361194610596,  Discriminator_loss: 1.1795200109481812\n",
      "Time for epoch 93 is 0.5023503303527832 sec Generator Loss: 1.0687191486358643,  Discriminator_loss: 1.1666525602340698\n",
      "Time for epoch 94 is 0.457594633102417 sec Generator Loss: 1.049403190612793,  Discriminator_loss: 1.186860203742981\n",
      "Time for epoch 95 is 0.45903658866882324 sec Generator Loss: 1.0545289516448975,  Discriminator_loss: 1.1617571115493774\n",
      "Time for epoch 96 is 0.4998307228088379 sec Generator Loss: 1.0378310680389404,  Discriminator_loss: 1.1757177114486694\n",
      "Time for epoch 97 is 0.4652082920074463 sec Generator Loss: 1.0394566059112549,  Discriminator_loss: 1.1857205629348755\n",
      "Time for epoch 98 is 0.4563934803009033 sec Generator Loss: 1.0492349863052368,  Discriminator_loss: 1.181291103363037\n",
      "Time for epoch 99 is 0.450423002243042 sec Generator Loss: 1.0347721576690674,  Discriminator_loss: 1.1957533359527588\n",
      "Time for epoch 100 is 0.44803881645202637 sec Generator Loss: 1.0182803869247437,  Discriminator_loss: 1.20639967918396\n",
      "Time for epoch 101 is 0.44934725761413574 sec Generator Loss: 1.0280152559280396,  Discriminator_loss: 1.1817512512207031\n",
      "Time for epoch 102 is 0.4451897144317627 sec Generator Loss: 1.0070600509643555,  Discriminator_loss: 1.1901236772537231\n",
      "Time for epoch 103 is 0.47112607955932617 sec Generator Loss: 1.0150794982910156,  Discriminator_loss: 1.2014631032943726\n",
      "Time for epoch 104 is 0.45648837089538574 sec Generator Loss: 1.0091618299484253,  Discriminator_loss: 1.2063124179840088\n",
      "Time for epoch 105 is 0.4608278274536133 sec Generator Loss: 0.993232011795044,  Discriminator_loss: 1.227162480354309\n",
      "Time for epoch 106 is 0.45772838592529297 sec Generator Loss: 0.9907417893409729,  Discriminator_loss: 1.2233206033706665\n",
      "Time for epoch 107 is 0.46123194694519043 sec Generator Loss: 1.0144968032836914,  Discriminator_loss: 1.2007535696029663\n",
      "Time for epoch 108 is 0.4556858539581299 sec Generator Loss: 1.0090970993041992,  Discriminator_loss: 1.1828813552856445\n",
      "Time for epoch 109 is 0.4467964172363281 sec Generator Loss: 1.0002703666687012,  Discriminator_loss: 1.2029666900634766\n",
      "Time for epoch 110 is 0.4509854316711426 sec Generator Loss: 0.9760968089103699,  Discriminator_loss: 1.2241331338882446\n",
      "Time for epoch 111 is 0.4932725429534912 sec Generator Loss: 0.9944643378257751,  Discriminator_loss: 1.2150028944015503\n",
      "Time for epoch 112 is 0.4724080562591553 sec Generator Loss: 0.9751927256584167,  Discriminator_loss: 1.2115548849105835\n",
      "Time for epoch 113 is 0.46233558654785156 sec Generator Loss: 0.9905520677566528,  Discriminator_loss: 1.2161927223205566\n",
      "Time for epoch 114 is 0.5209634304046631 sec Generator Loss: 0.9810452461242676,  Discriminator_loss: 1.2164020538330078\n",
      "Time for epoch 115 is 0.46490931510925293 sec Generator Loss: 0.9675683975219727,  Discriminator_loss: 1.2090812921524048\n",
      "Time for epoch 116 is 0.46131372451782227 sec Generator Loss: 0.9740440845489502,  Discriminator_loss: 1.2401795387268066\n",
      "Time for epoch 117 is 0.4505476951599121 sec Generator Loss: 0.9814678430557251,  Discriminator_loss: 1.225167155265808\n",
      "Time for epoch 118 is 0.4685516357421875 sec Generator Loss: 0.9676276445388794,  Discriminator_loss: 1.2294566631317139\n",
      "Time for epoch 119 is 0.46471214294433594 sec Generator Loss: 0.9612250328063965,  Discriminator_loss: 1.223831295967102\n",
      "Time for epoch 120 is 0.4652869701385498 sec Generator Loss: 0.9627639651298523,  Discriminator_loss: 1.228356122970581\n",
      "Time for epoch 121 is 0.4527106285095215 sec Generator Loss: 0.9692451357841492,  Discriminator_loss: 1.2284629344940186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 122 is 0.45910120010375977 sec Generator Loss: 0.9707897901535034,  Discriminator_loss: 1.227319598197937\n",
      "Time for epoch 123 is 0.46698451042175293 sec Generator Loss: 0.9757958054542542,  Discriminator_loss: 1.2373936176300049\n",
      "Time for epoch 124 is 0.47123241424560547 sec Generator Loss: 0.951871395111084,  Discriminator_loss: 1.2469398975372314\n",
      "Time for epoch 125 is 0.46710991859436035 sec Generator Loss: 0.9464617371559143,  Discriminator_loss: 1.2604292631149292\n",
      "Time for epoch 126 is 0.45687365531921387 sec Generator Loss: 0.9560791850090027,  Discriminator_loss: 1.2458451986312866\n",
      "Time for epoch 127 is 0.49667906761169434 sec Generator Loss: 0.939408004283905,  Discriminator_loss: 1.2611433267593384\n",
      "Time for epoch 128 is 0.4532623291015625 sec Generator Loss: 0.9585206508636475,  Discriminator_loss: 1.2420940399169922\n",
      "Time for epoch 129 is 0.4644484519958496 sec Generator Loss: 0.9619361758232117,  Discriminator_loss: 1.2442429065704346\n",
      "Time for epoch 130 is 0.4508028030395508 sec Generator Loss: 0.959658682346344,  Discriminator_loss: 1.2755942344665527\n",
      "Time for epoch 131 is 0.5137763023376465 sec Generator Loss: 0.9546048045158386,  Discriminator_loss: 1.2651420831680298\n",
      "Time for epoch 132 is 0.44631052017211914 sec Generator Loss: 0.942357063293457,  Discriminator_loss: 1.2640575170516968\n",
      "Time for epoch 133 is 0.45210838317871094 sec Generator Loss: 0.941862165927887,  Discriminator_loss: 1.2483805418014526\n",
      "Time for epoch 134 is 0.4660062789916992 sec Generator Loss: 0.9486891031265259,  Discriminator_loss: 1.271223545074463\n",
      "Time for epoch 135 is 0.5138182640075684 sec Generator Loss: 0.9517959356307983,  Discriminator_loss: 1.2528074979782104\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "data_gen_train, data_gen_test = get_gen_train_test(df, n_sequence, n_batch)\n",
    "\n",
    "generator = make_generator_model(n_sequence, n_features)\n",
    "discriminator=make_discriminator_model(n_features)\n",
    "\n",
    "learning_rate=1e-4\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(sequences, sequences_end):\n",
    "  return train_step_def(sequences, sequences_end)\n",
    "\n",
    "@tf.function\n",
    "def test_step(sequences, sequences_end):\n",
    "  return test_step_def(sequences, sequences_end)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'+stock_code\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "history, history_val = train(data_gen_train, data_gen_test, EPOCHS)\n",
    "\n",
    "plot_history(history, history_val)\n",
    "plot_frame(*data_gen_test[0], generator)\n",
    "\n",
    "print(\"[MSE Baseline] train:\",mean_squared_error(data_gen_train),\" test:\", mean_squared_error(data_gen_test))\n",
    "now = datetime.datetime.now()\n",
    "delta = now - start_time\n",
    "print(\"Delta time:\", delta)\n",
    "generator.save(\"lstm_ffnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a78c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
