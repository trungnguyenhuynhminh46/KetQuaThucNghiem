{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0627faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab6eea",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cfe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU, BatchNormalization, Bidirectional, LSTM, concatenate, Flatten\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "from pandas_datareader import data as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ea8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframe(df):\n",
    "  for column_name in df.columns:\n",
    "    plt.figure()\n",
    "    plt.title(column_name)\n",
    "    plt.plot(df[column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64236b5a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94817d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Ma(df, window=5):\n",
    "    for i in range(window, df.shape[0]):\n",
    "        sum = 0.0\n",
    "        for k in range(1, window+1):\n",
    "            sum += df.iloc[i-k, 4]\n",
    "        df.loc[df.index[i], 'Ma'] = np.round(sum/window, 6)\n",
    "    return df[window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68bdddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standarized_TimeseriesGenerator(tf.keras.preprocessing.sequence.TimeseriesGenerator):\n",
    "  def __getitem__(self, index):\n",
    "    samples, targets  = super(Standarized_TimeseriesGenerator, self).__getitem__(index)\n",
    "    mean = samples.mean(axis=1)\n",
    "    std = samples.std(axis=1)\n",
    "    samples = (samples - mean[:,None,:])/std[:,None,:]\n",
    "    targets = (targets - mean)/std\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20efe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_train_test(dataframe, n_sequence, n_batch):\n",
    "    data = dataframe.drop(columns='Date').to_numpy()\n",
    "    targets = data\n",
    "    n_samples = data.shape[0]\n",
    "    train_test_split=int(n_samples*0.9)\n",
    "\n",
    "    data_gen_train = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = 0,\n",
    "                                end_index = train_test_split,\n",
    "                                shuffle = True)\n",
    "    data_gen_test = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = train_test_split,\n",
    "                                end_index = n_samples-1)\n",
    "\n",
    "    return data_gen_train, data_gen_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596c86e",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36e1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X, lenght = 5):\n",
    "    squared_error = 0\n",
    "    for i in range(0, X.shape[0] - lenght):\n",
    "        x = X[i:i+lenght]\n",
    "        mean = x.mean()\n",
    "        std = x.std()\n",
    "        x = (x - mean)/std\n",
    "        y = (X[i+lenght] - mean)/std\n",
    "        squared_error += np.square(x[-1]-y)\n",
    "    return squared_error/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5d3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generated data\n",
    "def mean_squared_error(dataset):\n",
    "    mse=0\n",
    "    for X_batch, y_batch in dataset:\n",
    "        mse += np.mean(np.square(X_batch[:, -1, 3:4]-y_batch[:, 3:4]))\n",
    "    mse /= len(dataset)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82bae3",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63452d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3]))\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def mape(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])/y_true[:,3]))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3])))\n",
    "def ar(y_true, y_pred):\n",
    "    mask = tf.cast(y_pred[1:,3] > y_true[:-1,3],tf.float32)\n",
    "    return tf.reduce_mean((y_true[1:,3]-y_true[:-1,3])*mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d7d3",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0763cd9",
   "metadata": {},
   "source": [
    "## Perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e3849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(x, y, fake_output):\n",
    "    a1=0.01\n",
    "    g_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_mse = tf.keras.losses.MSE(x, y)\n",
    "    return a1*g_mse + (1-a1)*g_loss, g_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda87fb",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a10957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(n_sequence, n_features):\n",
    "    inputs = Input(shape=(n_sequence, n_features,))\n",
    "    \n",
    "    # Define the first Densely Connected Bidirectional LSTM layer\n",
    "    lstm_1 = Bidirectional(LSTM(units=10, return_sequences=True, activation=None, kernel_initializer='random_normal', dropout=0.3))(inputs)\n",
    "    lstm_1_batch_norm = BatchNormalization()(lstm_1)\n",
    "    lstm_1_LRelu = LeakyReLU(alpha=0.3)(lstm_1_batch_norm)\n",
    "    lstm_1_dropout = Dropout(0.3)(lstm_1_LRelu)\n",
    "    \n",
    "    # Define the second Densely Connected Bidirectional LSTM layer\n",
    "    lstm_2_input = concatenate([lstm_1, inputs], axis=2)\n",
    "    lstm_2 = Bidirectional(LSTM(units=10, return_sequences=False, activation=None, kernel_initializer='random_normal', dropout=0.3))(lstm_2_input)\n",
    "    lstm_2_batch_norm = BatchNormalization()(lstm_2)\n",
    "    lstm_2_LRelu = LeakyReLU(alpha=0.3)(lstm_2_batch_norm)\n",
    "    lstm_2_dropout = Dropout(0.3)(lstm_2_LRelu)\n",
    "    \n",
    "    # Define the output layer\n",
    "    output_dense = Dense(n_features, activation=None)(lstm_2_dropout)\n",
    "    output = LeakyReLU(alpha=0.3)(output_dense)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(loss=None, metrics=[mse, mae, mape, rmse, ar])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73434c04",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbabdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=72, input_shape=((n_sequence+1) * n_features,), activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(tf.keras.layers.GaussianNoise(stddev=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=100, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=10, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1 ,activation='sigmoid'))\n",
    "    model.compile(loss=discriminator_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29f52d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5acba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_def(sequences, sequences_end):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_prediction = generator(sequences, training=True)\n",
    "\n",
    "        sequences_true = tf.concat((sequences, sequences_end[:, None, :]), axis=1)\n",
    "        sequences_fake = tf.concat((sequences, generated_prediction[:, None, :]), axis=1)\n",
    "\n",
    "        real_output = discriminator(sequences_true, training=True)\n",
    "        fake_output = discriminator(sequences_fake, training=True)\n",
    "\n",
    "        gen_loss, gen_mse_loss = generator_loss(generated_prediction, \n",
    "                                                sequences_end, \n",
    "                                                fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)\n",
    "\n",
    "def test_step_def(sequences, sequences_end):\n",
    "    generated_prediction = generator(sequences, training=False)\n",
    "\n",
    "    sequences_true = tf.concat((sequences, sequences_end[:,None,:]), axis=1)\n",
    "    sequences_fake = tf.concat((sequences, generated_prediction[:,None,:]), axis=1)\n",
    "\n",
    "    real_output = discriminator(sequences_true, training=False)\n",
    "    fake_output = discriminator(sequences_fake, training=False)\n",
    "\n",
    "    gen_loss, gen_mse_loss = generator_loss(generated_prediction, sequences_end, fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81363ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, dataset_val, epochs):\n",
    "    history = np.empty(shape = (8, epochs))\n",
    "    history_val = np.empty(shape = (8, epochs))\n",
    "    len_dataset = len(dataset)\n",
    "    len_dataset_val = len(dataset_val)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        cur_dis_loss = 0\n",
    "        cur_gen_loss = 0\n",
    "        cur_gen_mse_loss = 0\n",
    "        for sequence_batch, sequence_end_batch in dataset:\n",
    "            aux_cur_losses = train_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                      tf.cast(sequence_end_batch, tf.float32))\n",
    "            cur_gen_loss += aux_cur_losses[0]/len_dataset\n",
    "            cur_dis_loss += aux_cur_losses[1]/len_dataset\n",
    "            cur_gen_mse_loss += aux_cur_losses[2]/len_dataset\n",
    "        cur_gen_metrics = generator.evaluate(dataset,verbose=False)[1:]\n",
    "\n",
    "        history[:, epoch] = cur_gen_loss, cur_dis_loss, cur_gen_mse_loss, *cur_gen_metrics\n",
    "\n",
    "        cur_gen_metrics_val = generator.evaluate(dataset_val,verbose=False)[1: ]\n",
    "\n",
    "        cur_gen_loss_val = 0\n",
    "        cur_dis_loss_val = 0\n",
    "        cur_gen_mse_loss_val = 0\n",
    "        for sequence_batch, sequence_end_batch in dataset_val:\n",
    "            aux_cur_losses_val = test_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                         tf.cast(sequence_end_batch, tf.float32))\n",
    "            cur_gen_loss_val += aux_cur_losses_val[0]/len_dataset_val\n",
    "            cur_dis_loss_val += aux_cur_losses_val[1]/len_dataset_val\n",
    "            cur_gen_mse_loss_val += aux_cur_losses_val[2]/len_dataset_val\n",
    "    \n",
    "\n",
    "\n",
    "        history_val[:, epoch] = cur_gen_loss_val, cur_dis_loss_val, cur_gen_mse_loss_val, *cur_gen_metrics_val\n",
    "\n",
    "        print ('Time for epoch {} is {} sec Generator Loss: {},  Discriminator_loss: {}'\n",
    "               .format(epoch + 1, time.time()-start, cur_gen_loss, cur_dis_loss))\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "#         if(cur_gen_loss > 0.85):\n",
    "#                 break;\n",
    "    return history, history_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca9168",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5119da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history_val):\n",
    "    metrics = [\"gen_loss\",\"dis_loss\",\"gen_mse_loss\", 'mse','mae','mape','rmse','ar']\n",
    "    for i, metric_name in enumerate(metrics):  \n",
    "        plt.figure()\n",
    "        plt.title(metric_name)\n",
    "        plt.plot(history[i], label='train')\n",
    "        plt.plot(history_val[i], label='test')\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aae39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(sequence, target, model):\n",
    "    y_pred = model.predict(sequence)[...,3]\n",
    "    y_true = target[...,3]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"closing price\")\n",
    "    plt.plot(y_true, label=\"true\")\n",
    "    plt.plot(y_pred, label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6a41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results(history):\n",
    "    min_index = np.argmin(history[3, :])\n",
    "    return history[:, min_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc15b2",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef9e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "n_sequence = window\n",
    "n_features = 7\n",
    "n_batch = 50\n",
    "EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb9e4a",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13605d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010-07-07</td>\n",
       "      <td>1.093333</td>\n",
       "      <td>1.108667</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>1.053333</td>\n",
       "      <td>103825500</td>\n",
       "      <td>1.399867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010-07-08</td>\n",
       "      <td>1.076000</td>\n",
       "      <td>1.168000</td>\n",
       "      <td>1.038000</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>1.164000</td>\n",
       "      <td>115671000</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2010-07-09</td>\n",
       "      <td>1.172000</td>\n",
       "      <td>1.193333</td>\n",
       "      <td>1.103333</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>60759000</td>\n",
       "      <td>1.207067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2010-07-12</td>\n",
       "      <td>1.196667</td>\n",
       "      <td>1.204667</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>1.136667</td>\n",
       "      <td>1.136667</td>\n",
       "      <td>33037500</td>\n",
       "      <td>1.146267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010-07-13</td>\n",
       "      <td>1.159333</td>\n",
       "      <td>1.242667</td>\n",
       "      <td>1.126667</td>\n",
       "      <td>1.209333</td>\n",
       "      <td>1.209333</td>\n",
       "      <td>40201500</td>\n",
       "      <td>1.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>210.733337</td>\n",
       "      <td>217.166672</td>\n",
       "      <td>207.523331</td>\n",
       "      <td>215.326660</td>\n",
       "      <td>215.326660</td>\n",
       "      <td>99519000</td>\n",
       "      <td>217.591333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>214.330002</td>\n",
       "      <td>222.029999</td>\n",
       "      <td>213.666672</td>\n",
       "      <td>220.589996</td>\n",
       "      <td>220.589996</td>\n",
       "      <td>68596800</td>\n",
       "      <td>219.138666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>224.836670</td>\n",
       "      <td>227.133331</td>\n",
       "      <td>220.266663</td>\n",
       "      <td>221.229996</td>\n",
       "      <td>221.229996</td>\n",
       "      <td>96835800</td>\n",
       "      <td>219.529999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>220.333328</td>\n",
       "      <td>223.300003</td>\n",
       "      <td>218.333328</td>\n",
       "      <td>221.996674</td>\n",
       "      <td>221.996674</td>\n",
       "      <td>68732400</td>\n",
       "      <td>217.442664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>232.199997</td>\n",
       "      <td>222.786667</td>\n",
       "      <td>231.593338</td>\n",
       "      <td>231.593338</td>\n",
       "      <td>128538000</td>\n",
       "      <td>218.517999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2641 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "5    2010-07-07    1.093333    1.108667    0.998667    1.053333    1.053333   \n",
       "6    2010-07-08    1.076000    1.168000    1.038000    1.164000    1.164000   \n",
       "7    2010-07-09    1.172000    1.193333    1.103333    1.160000    1.160000   \n",
       "8    2010-07-12    1.196667    1.204667    1.133333    1.136667    1.136667   \n",
       "9    2010-07-13    1.159333    1.242667    1.126667    1.209333    1.209333   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2641 2020-12-23  210.733337  217.166672  207.523331  215.326660  215.326660   \n",
       "2642 2020-12-24  214.330002  222.029999  213.666672  220.589996  220.589996   \n",
       "2643 2020-12-28  224.836670  227.133331  220.266663  221.229996  221.229996   \n",
       "2644 2020-12-29  220.333328  223.300003  218.333328  221.996674  221.996674   \n",
       "2645 2020-12-30  224.000000  232.199997  222.786667  231.593338  231.593338   \n",
       "\n",
       "         Volume          Ma  \n",
       "5     103825500    1.399867  \n",
       "6     115671000    1.292000  \n",
       "7      60759000    1.207067  \n",
       "8      33037500    1.146267  \n",
       "9      40201500    1.117600  \n",
       "...         ...         ...  \n",
       "2641   99519000  217.591333  \n",
       "2642   68596800  219.138666  \n",
       "2643   96835800  219.529999  \n",
       "2644   68732400  217.442664  \n",
       "2645  128538000  218.517999  \n",
       "\n",
       "[2641 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_code = \"TSLA\"\n",
    "start = dt.datetime(2000, 1, 1)\n",
    "end = dt.datetime(2020, 12, 31)\n",
    "raw_data = pdr.get_data_yahoo(stock_code, start, end,threads=False, proxy=\"http://127.0.0.1:7890\")\n",
    "df = raw_data.dropna();\n",
    "df = df.reset_index(level=0)\n",
    "df = add_Ma(df, window)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 7)]       0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 5, 20)        1440        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 5, 27)        0           ['bidirectional[0][0]',          \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 20)          3040        ['concatenate[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 20)          80          ['bidirectional_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 20)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20)           0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 7)            147         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 7)            0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,707\n",
      "Trainable params: 4,667\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n",
      "Time for epoch 1 is 12.4927237033844 sec Generator Loss: 0.7480176687240601,  Discriminator_loss: 1.3837450742721558\n",
      "Time for epoch 2 is 0.533353328704834 sec Generator Loss: 0.7421784996986389,  Discriminator_loss: 1.3704228401184082\n",
      "Time for epoch 3 is 0.5550196170806885 sec Generator Loss: 0.7394919395446777,  Discriminator_loss: 1.3575024604797363\n",
      "Time for epoch 4 is 0.5748484134674072 sec Generator Loss: 0.7373635172843933,  Discriminator_loss: 1.3390016555786133\n",
      "Time for epoch 5 is 0.6022608280181885 sec Generator Loss: 0.7356493473052979,  Discriminator_loss: 1.314850926399231\n",
      "Time for epoch 6 is 0.5359721183776855 sec Generator Loss: 0.7440941333770752,  Discriminator_loss: 1.2907878160476685\n",
      "Time for epoch 7 is 0.5621931552886963 sec Generator Loss: 0.7719337344169617,  Discriminator_loss: 1.2564566135406494\n",
      "Time for epoch 8 is 0.5388352870941162 sec Generator Loss: 0.7964439392089844,  Discriminator_loss: 1.2201817035675049\n",
      "Time for epoch 9 is 0.5902190208435059 sec Generator Loss: 0.8295126557350159,  Discriminator_loss: 1.194821834564209\n",
      "Time for epoch 10 is 0.5927975177764893 sec Generator Loss: 0.8613422513008118,  Discriminator_loss: 1.1634098291397095\n",
      "Time for epoch 11 is 0.6361105442047119 sec Generator Loss: 0.8948333859443665,  Discriminator_loss: 1.1579347848892212\n",
      "Time for epoch 12 is 0.597318172454834 sec Generator Loss: 0.9148182272911072,  Discriminator_loss: 1.1515055894851685\n",
      "Time for epoch 13 is 0.5517661571502686 sec Generator Loss: 0.9542851448059082,  Discriminator_loss: 1.1333794593811035\n",
      "Time for epoch 14 is 0.5233213901519775 sec Generator Loss: 0.9577577114105225,  Discriminator_loss: 1.1447144746780396\n",
      "Time for epoch 15 is 0.5460019111633301 sec Generator Loss: 0.9761422872543335,  Discriminator_loss: 1.1547355651855469\n",
      "Time for epoch 16 is 0.5522670745849609 sec Generator Loss: 0.969085156917572,  Discriminator_loss: 1.177420973777771\n",
      "Time for epoch 17 is 0.5989658832550049 sec Generator Loss: 0.9579827189445496,  Discriminator_loss: 1.188507080078125\n",
      "Time for epoch 18 is 0.5997676849365234 sec Generator Loss: 0.9678266048431396,  Discriminator_loss: 1.1884567737579346\n",
      "Time for epoch 19 is 0.528221607208252 sec Generator Loss: 1.001865029335022,  Discriminator_loss: 1.1864784955978394\n",
      "Time for epoch 20 is 0.5984029769897461 sec Generator Loss: 0.9947933554649353,  Discriminator_loss: 1.2007240056991577\n",
      "Time for epoch 21 is 0.6602222919464111 sec Generator Loss: 0.9816131591796875,  Discriminator_loss: 1.208742618560791\n",
      "Time for epoch 22 is 0.7299144268035889 sec Generator Loss: 0.9756953716278076,  Discriminator_loss: 1.2262901067733765\n",
      "Time for epoch 23 is 0.5216264724731445 sec Generator Loss: 0.9877210855484009,  Discriminator_loss: 1.222794771194458\n",
      "Time for epoch 24 is 0.5837430953979492 sec Generator Loss: 0.9690457582473755,  Discriminator_loss: 1.2367897033691406\n",
      "Time for epoch 25 is 0.7349283695220947 sec Generator Loss: 0.9552744030952454,  Discriminator_loss: 1.2296230792999268\n",
      "Time for epoch 26 is 0.6752464771270752 sec Generator Loss: 0.9538605213165283,  Discriminator_loss: 1.2238013744354248\n",
      "Time for epoch 27 is 0.5835163593292236 sec Generator Loss: 0.9659096002578735,  Discriminator_loss: 1.2377971410751343\n",
      "Time for epoch 28 is 0.736652135848999 sec Generator Loss: 0.9836747646331787,  Discriminator_loss: 1.236054539680481\n",
      "Time for epoch 29 is 0.6446089744567871 sec Generator Loss: 0.9411619305610657,  Discriminator_loss: 1.2385138273239136\n",
      "Time for epoch 30 is 0.675316333770752 sec Generator Loss: 0.949776828289032,  Discriminator_loss: 1.226510763168335\n",
      "Time for epoch 31 is 0.6992340087890625 sec Generator Loss: 0.9440529346466064,  Discriminator_loss: 1.2466213703155518\n",
      "Time for epoch 32 is 0.6618409156799316 sec Generator Loss: 0.9364224672317505,  Discriminator_loss: 1.238518238067627\n",
      "Time for epoch 33 is 0.7037169933319092 sec Generator Loss: 0.9369837045669556,  Discriminator_loss: 1.2417833805084229\n",
      "Time for epoch 34 is 0.6498496532440186 sec Generator Loss: 0.9387950897216797,  Discriminator_loss: 1.2307982444763184\n",
      "Time for epoch 35 is 0.5552358627319336 sec Generator Loss: 0.9368783235549927,  Discriminator_loss: 1.2343488931655884\n",
      "Time for epoch 36 is 0.705312967300415 sec Generator Loss: 0.9266418814659119,  Discriminator_loss: 1.2449990510940552\n",
      "Time for epoch 37 is 0.7738888263702393 sec Generator Loss: 0.9365056753158569,  Discriminator_loss: 1.2425777912139893\n",
      "Time for epoch 38 is 0.6971311569213867 sec Generator Loss: 0.943732738494873,  Discriminator_loss: 1.219221830368042\n",
      "Time for epoch 39 is 0.6118130683898926 sec Generator Loss: 0.9445176720619202,  Discriminator_loss: 1.2319748401641846\n",
      "Time for epoch 40 is 0.6414709091186523 sec Generator Loss: 0.9351726174354553,  Discriminator_loss: 1.233147144317627\n",
      "Time for epoch 41 is 0.652841329574585 sec Generator Loss: 0.937320351600647,  Discriminator_loss: 1.2299033403396606\n",
      "Time for epoch 42 is 0.56402587890625 sec Generator Loss: 0.9186235666275024,  Discriminator_loss: 1.2324702739715576\n",
      "Time for epoch 43 is 0.5663814544677734 sec Generator Loss: 0.9405423998832703,  Discriminator_loss: 1.239498496055603\n",
      "Time for epoch 44 is 0.6121141910552979 sec Generator Loss: 0.9211134910583496,  Discriminator_loss: 1.2252767086029053\n",
      "Time for epoch 45 is 0.5549826622009277 sec Generator Loss: 0.9229484796524048,  Discriminator_loss: 1.2312192916870117\n",
      "Time for epoch 46 is 0.6213412284851074 sec Generator Loss: 0.9231200218200684,  Discriminator_loss: 1.2323518991470337\n",
      "Time for epoch 47 is 0.5577998161315918 sec Generator Loss: 0.9330509901046753,  Discriminator_loss: 1.217676043510437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 48 is 0.5922591686248779 sec Generator Loss: 0.9390089511871338,  Discriminator_loss: 1.2183442115783691\n",
      "Time for epoch 49 is 0.5796794891357422 sec Generator Loss: 0.945986807346344,  Discriminator_loss: 1.214781403541565\n",
      "Time for epoch 50 is 0.626070499420166 sec Generator Loss: 0.9537145495414734,  Discriminator_loss: 1.2172067165374756\n",
      "Time for epoch 51 is 0.581104040145874 sec Generator Loss: 0.9376091957092285,  Discriminator_loss: 1.24082612991333\n",
      "Time for epoch 52 is 0.5668301582336426 sec Generator Loss: 0.9379256367683411,  Discriminator_loss: 1.2248536348342896\n",
      "Time for epoch 53 is 0.5718159675598145 sec Generator Loss: 0.9374288320541382,  Discriminator_loss: 1.2419445514678955\n",
      "Time for epoch 54 is 0.5111165046691895 sec Generator Loss: 0.9409018158912659,  Discriminator_loss: 1.2332963943481445\n",
      "Time for epoch 55 is 0.4652256965637207 sec Generator Loss: 0.9353492259979248,  Discriminator_loss: 1.2501813173294067\n",
      "Time for epoch 56 is 0.4655890464782715 sec Generator Loss: 0.9282280206680298,  Discriminator_loss: 1.2550323009490967\n",
      "Time for epoch 57 is 0.5782208442687988 sec Generator Loss: 0.91059410572052,  Discriminator_loss: 1.285051941871643\n",
      "Time for epoch 58 is 0.5144810676574707 sec Generator Loss: 0.9152045249938965,  Discriminator_loss: 1.2694803476333618\n",
      "Time for epoch 59 is 0.5204465389251709 sec Generator Loss: 0.9153635501861572,  Discriminator_loss: 1.2621195316314697\n",
      "Time for epoch 60 is 0.5194966793060303 sec Generator Loss: 0.910275936126709,  Discriminator_loss: 1.2818117141723633\n",
      "Time for epoch 61 is 0.5734615325927734 sec Generator Loss: 0.8886536955833435,  Discriminator_loss: 1.3146679401397705\n",
      "Time for epoch 62 is 0.5315248966217041 sec Generator Loss: 0.8926832675933838,  Discriminator_loss: 1.3055261373519897\n",
      "Time for epoch 63 is 0.5922141075134277 sec Generator Loss: 0.9003087282180786,  Discriminator_loss: 1.272348165512085\n",
      "Time for epoch 64 is 0.5923805236816406 sec Generator Loss: 0.882155179977417,  Discriminator_loss: 1.281227469444275\n",
      "Time for epoch 65 is 0.6074085235595703 sec Generator Loss: 0.8487989902496338,  Discriminator_loss: 1.3387024402618408\n",
      "Time for epoch 66 is 0.651137113571167 sec Generator Loss: 0.8556472659111023,  Discriminator_loss: 1.3529064655303955\n",
      "Time for epoch 67 is 0.6330795288085938 sec Generator Loss: 0.8684656023979187,  Discriminator_loss: 1.327290654182434\n",
      "Time for epoch 68 is 0.6359050273895264 sec Generator Loss: 0.8793656229972839,  Discriminator_loss: 1.2930406332015991\n",
      "Time for epoch 69 is 0.6036486625671387 sec Generator Loss: 0.8788971304893494,  Discriminator_loss: 1.2728619575500488\n",
      "Time for epoch 70 is 0.5313961505889893 sec Generator Loss: 0.8621319532394409,  Discriminator_loss: 1.2939270734786987\n",
      "Time for epoch 71 is 0.5810701847076416 sec Generator Loss: 0.8721697330474854,  Discriminator_loss: 1.3062043190002441\n",
      "Time for epoch 72 is 0.5233371257781982 sec Generator Loss: 0.8689394593238831,  Discriminator_loss: 1.3254642486572266\n",
      "Time for epoch 73 is 0.5663332939147949 sec Generator Loss: 0.8649231195449829,  Discriminator_loss: 1.3368196487426758\n",
      "Time for epoch 74 is 0.53729248046875 sec Generator Loss: 0.8522210717201233,  Discriminator_loss: 1.3416869640350342\n",
      "Time for epoch 75 is 0.5331578254699707 sec Generator Loss: 0.8651828169822693,  Discriminator_loss: 1.3196712732315063\n",
      "Time for epoch 76 is 0.5435066223144531 sec Generator Loss: 0.8839453458786011,  Discriminator_loss: 1.2786314487457275\n",
      "Time for epoch 77 is 0.5950469970703125 sec Generator Loss: 0.8850711584091187,  Discriminator_loss: 1.2769981622695923\n",
      "Time for epoch 78 is 0.5587396621704102 sec Generator Loss: 0.8462048768997192,  Discriminator_loss: 1.3098106384277344\n",
      "Time for epoch 79 is 0.5316977500915527 sec Generator Loss: 0.8193099498748779,  Discriminator_loss: 1.3696975708007812\n",
      "Time for epoch 80 is 0.5256557464599609 sec Generator Loss: 0.8327494263648987,  Discriminator_loss: 1.3686578273773193\n",
      "Time for epoch 81 is 0.5280880928039551 sec Generator Loss: 0.8447417616844177,  Discriminator_loss: 1.3386139869689941\n",
      "Time for epoch 82 is 0.521845817565918 sec Generator Loss: 0.8585288524627686,  Discriminator_loss: 1.3119980096817017\n",
      "Time for epoch 83 is 0.5183372497558594 sec Generator Loss: 0.8487222790718079,  Discriminator_loss: 1.312238097190857\n",
      "Time for epoch 84 is 0.5234246253967285 sec Generator Loss: 0.8503594994544983,  Discriminator_loss: 1.3030468225479126\n",
      "Time for epoch 85 is 0.5250048637390137 sec Generator Loss: 0.8621894717216492,  Discriminator_loss: 1.317338466644287\n",
      "Time for epoch 86 is 0.5441875457763672 sec Generator Loss: 0.8623997569084167,  Discriminator_loss: 1.3082414865493774\n",
      "Time for epoch 87 is 0.5797369480133057 sec Generator Loss: 0.8528529405593872,  Discriminator_loss: 1.3268877267837524\n",
      "Time for epoch 88 is 0.5901997089385986 sec Generator Loss: 0.837007999420166,  Discriminator_loss: 1.338938593864441\n",
      "Time for epoch 89 is 0.5806224346160889 sec Generator Loss: 0.8475198149681091,  Discriminator_loss: 1.3386948108673096\n",
      "Time for epoch 90 is 0.5574524402618408 sec Generator Loss: 0.8538455367088318,  Discriminator_loss: 1.3291740417480469\n",
      "Time for epoch 91 is 0.5838437080383301 sec Generator Loss: 0.8615213632583618,  Discriminator_loss: 1.322206974029541\n",
      "Time for epoch 92 is 0.5641400814056396 sec Generator Loss: 0.8528614640235901,  Discriminator_loss: 1.314706563949585\n",
      "Time for epoch 93 is 0.6316745281219482 sec Generator Loss: 0.8511841893196106,  Discriminator_loss: 1.3106147050857544\n",
      "Time for epoch 94 is 0.5586757659912109 sec Generator Loss: 0.8266279697418213,  Discriminator_loss: 1.3466286659240723\n",
      "Time for epoch 95 is 0.5498948097229004 sec Generator Loss: 0.834507167339325,  Discriminator_loss: 1.3527075052261353\n",
      "Time for epoch 96 is 0.5342957973480225 sec Generator Loss: 0.8452709913253784,  Discriminator_loss: 1.3341975212097168\n",
      "Time for epoch 97 is 0.5341999530792236 sec Generator Loss: 0.8467970490455627,  Discriminator_loss: 1.3197993040084839\n",
      "Time for epoch 98 is 0.530886173248291 sec Generator Loss: 0.8550503253936768,  Discriminator_loss: 1.311349868774414\n",
      "Time for epoch 99 is 0.6204876899719238 sec Generator Loss: 0.8332163095474243,  Discriminator_loss: 1.3329499959945679\n",
      "Time for epoch 100 is 0.5625879764556885 sec Generator Loss: 0.8492545485496521,  Discriminator_loss: 1.3274788856506348\n",
      "Time for epoch 101 is 0.5736956596374512 sec Generator Loss: 0.8436256647109985,  Discriminator_loss: 1.322771430015564\n",
      "Time for epoch 102 is 0.6063113212585449 sec Generator Loss: 0.844582200050354,  Discriminator_loss: 1.3249279260635376\n",
      "Time for epoch 103 is 0.5354397296905518 sec Generator Loss: 0.8384986519813538,  Discriminator_loss: 1.3482412099838257\n",
      "Time for epoch 104 is 0.5766940116882324 sec Generator Loss: 0.8570877909660339,  Discriminator_loss: 1.3287714719772339\n",
      "Time for epoch 105 is 0.5769181251525879 sec Generator Loss: 0.8500621914863586,  Discriminator_loss: 1.3249077796936035\n",
      "Time for epoch 106 is 0.5366654396057129 sec Generator Loss: 0.8481800556182861,  Discriminator_loss: 1.3324764966964722\n",
      "Time for epoch 107 is 0.5798590183258057 sec Generator Loss: 0.8421570658683777,  Discriminator_loss: 1.3391202688217163\n",
      "Time for epoch 108 is 0.5354456901550293 sec Generator Loss: 0.8314536809921265,  Discriminator_loss: 1.3330144882202148\n",
      "Time for epoch 109 is 0.5451457500457764 sec Generator Loss: 0.8254470825195312,  Discriminator_loss: 1.3289514780044556\n",
      "Time for epoch 110 is 0.5467407703399658 sec Generator Loss: 0.8351420164108276,  Discriminator_loss: 1.3228065967559814\n",
      "Time for epoch 111 is 0.551936149597168 sec Generator Loss: 0.8383506536483765,  Discriminator_loss: 1.3285672664642334\n",
      "Time for epoch 112 is 0.6095237731933594 sec Generator Loss: 0.8323866724967957,  Discriminator_loss: 1.3279274702072144\n",
      "Time for epoch 113 is 0.7842979431152344 sec Generator Loss: 0.8341425657272339,  Discriminator_loss: 1.3267799615859985\n",
      "Time for epoch 114 is 0.6539614200592041 sec Generator Loss: 0.8364665508270264,  Discriminator_loss: 1.326465129852295\n",
      "Time for epoch 115 is 0.7071354389190674 sec Generator Loss: 0.8353104591369629,  Discriminator_loss: 1.3354551792144775\n",
      "Time for epoch 116 is 0.8524060249328613 sec Generator Loss: 0.8396849632263184,  Discriminator_loss: 1.3340697288513184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 117 is 0.745105504989624 sec Generator Loss: 0.8240736722946167,  Discriminator_loss: 1.33984375\n",
      "Time for epoch 118 is 0.7123496532440186 sec Generator Loss: 0.8374581933021545,  Discriminator_loss: 1.3293533325195312\n",
      "Time for epoch 119 is 0.5902442932128906 sec Generator Loss: 0.8269580006599426,  Discriminator_loss: 1.3321787118911743\n",
      "Time for epoch 120 is 0.6615056991577148 sec Generator Loss: 0.8410126566886902,  Discriminator_loss: 1.3171823024749756\n",
      "Time for epoch 121 is 0.6524198055267334 sec Generator Loss: 0.8266958594322205,  Discriminator_loss: 1.329844355583191\n",
      "Time for epoch 122 is 0.6531741619110107 sec Generator Loss: 0.833046019077301,  Discriminator_loss: 1.320014238357544\n",
      "Time for epoch 123 is 0.5835525989532471 sec Generator Loss: 0.8284838199615479,  Discriminator_loss: 1.3225306272506714\n",
      "Time for epoch 124 is 0.640810489654541 sec Generator Loss: 0.8250008225440979,  Discriminator_loss: 1.3309553861618042\n",
      "Time for epoch 125 is 0.5384891033172607 sec Generator Loss: 0.8346113562583923,  Discriminator_loss: 1.3183802366256714\n",
      "Time for epoch 126 is 0.5445113182067871 sec Generator Loss: 0.8415690660476685,  Discriminator_loss: 1.3240001201629639\n",
      "Time for epoch 127 is 0.5982482433319092 sec Generator Loss: 0.8346953988075256,  Discriminator_loss: 1.3220082521438599\n",
      "Time for epoch 128 is 0.5926997661590576 sec Generator Loss: 0.8351553678512573,  Discriminator_loss: 1.3223543167114258\n",
      "Time for epoch 129 is 0.6132314205169678 sec Generator Loss: 0.8358772397041321,  Discriminator_loss: 1.3263590335845947\n",
      "Time for epoch 130 is 0.559457540512085 sec Generator Loss: 0.8402692675590515,  Discriminator_loss: 1.3214317560195923\n",
      "Time for epoch 131 is 0.6037425994873047 sec Generator Loss: 0.8386134505271912,  Discriminator_loss: 1.3225699663162231\n",
      "Time for epoch 132 is 0.5990138053894043 sec Generator Loss: 0.8285028338432312,  Discriminator_loss: 1.3223812580108643\n",
      "Time for epoch 133 is 0.5788917541503906 sec Generator Loss: 0.8328526020050049,  Discriminator_loss: 1.3184829950332642\n",
      "Time for epoch 134 is 0.6870357990264893 sec Generator Loss: 0.8294653296470642,  Discriminator_loss: 1.3210349082946777\n",
      "Time for epoch 135 is 0.7149734497070312 sec Generator Loss: 0.8500640988349915,  Discriminator_loss: 1.3145767450332642\n",
      "Time for epoch 136 is 0.6388981342315674 sec Generator Loss: 0.8330997228622437,  Discriminator_loss: 1.3247642517089844\n",
      "Time for epoch 137 is 0.6446752548217773 sec Generator Loss: 0.8366305828094482,  Discriminator_loss: 1.3276838064193726\n",
      "Time for epoch 138 is 0.5544371604919434 sec Generator Loss: 0.8316710591316223,  Discriminator_loss: 1.3185632228851318\n",
      "Time for epoch 139 is 0.5801479816436768 sec Generator Loss: 0.8371952176094055,  Discriminator_loss: 1.3107353448867798\n",
      "Time for epoch 140 is 0.7210390567779541 sec Generator Loss: 0.8448444604873657,  Discriminator_loss: 1.3168421983718872\n",
      "Time for epoch 141 is 0.7486717700958252 sec Generator Loss: 0.8413825035095215,  Discriminator_loss: 1.319199562072754\n",
      "Time for epoch 142 is 0.7592473030090332 sec Generator Loss: 0.8381390571594238,  Discriminator_loss: 1.3170762062072754\n",
      "Time for epoch 143 is 0.6950662136077881 sec Generator Loss: 0.849600076675415,  Discriminator_loss: 1.3183008432388306\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "df = add_Ma(df)\n",
    "data_gen_train, data_gen_test = get_gen_train_test(df, n_sequence, n_batch)\n",
    "\n",
    "generator = make_generator_model(n_sequence, n_features)\n",
    "discriminator=make_discriminator_model(n_features)\n",
    "\n",
    "learning_rate=1e-4\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(sequences, sequences_end):\n",
    "  return train_step_def(sequences, sequences_end)\n",
    "\n",
    "@tf.function\n",
    "def test_step(sequences, sequences_end):\n",
    "  return test_step_def(sequences, sequences_end)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'+stock_code\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "history, history_val = train(data_gen_train, data_gen_test, EPOCHS)\n",
    "\n",
    "plot_history(history, history_val)\n",
    "plot_frame(*data_gen_test[0], generator)\n",
    "\n",
    "print(\"[MSE Baseline] train:\",mean_squared_error(data_gen_train),\" test:\", mean_squared_error(data_gen_test))\n",
    "now = datetime.datetime.now()\n",
    "delta = now - start_time\n",
    "print(\"Delta time:\", delta)\n",
    "generator.save(\"dc_bi_lstm_ffnn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
