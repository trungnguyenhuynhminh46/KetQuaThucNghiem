{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0627faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab6eea",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cfe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU, BatchNormalization, Bidirectional, LSTM, concatenate, Flatten\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "from pandas_datareader import data as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ea8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframe(df):\n",
    "  for column_name in df.columns:\n",
    "    plt.figure()\n",
    "    plt.title(column_name)\n",
    "    plt.plot(df[column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64236b5a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94817d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Ma(df, window=5):\n",
    "    for i in range(window, df.shape[0]):\n",
    "        sum = 0.0\n",
    "        for k in range(1, window+1):\n",
    "            sum += df.iloc[i-k, 4]\n",
    "        df.loc[df.index[i], 'Ma'] = np.round(sum/window, 6)\n",
    "    return df[window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68bdddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standarized_TimeseriesGenerator(tf.keras.preprocessing.sequence.TimeseriesGenerator):\n",
    "  def __getitem__(self, index):\n",
    "    samples, targets  = super(Standarized_TimeseriesGenerator, self).__getitem__(index)\n",
    "    mean = samples.mean(axis=1)\n",
    "    std = samples.std(axis=1)\n",
    "    samples = (samples - mean[:,None,:])/std[:,None,:]\n",
    "    targets = (targets - mean)/std\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20efe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_train_test(dataframe, n_sequence, n_batch):\n",
    "    data = dataframe.drop(columns='Date').to_numpy()\n",
    "    targets = data\n",
    "    n_samples = data.shape[0]\n",
    "    train_test_split=int(n_samples*0.9)\n",
    "\n",
    "    data_gen_train = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = 0,\n",
    "                                end_index = train_test_split,\n",
    "                                shuffle = True)\n",
    "    data_gen_test = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = train_test_split,\n",
    "                                end_index = n_samples-1)\n",
    "\n",
    "    return data_gen_train, data_gen_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596c86e",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36e1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X, lenght = 5):\n",
    "    squared_error = 0\n",
    "    for i in range(0, X.shape[0] - lenght):\n",
    "        x = X[i:i+lenght]\n",
    "        mean = x.mean()\n",
    "        std = x.std()\n",
    "        x = (x - mean)/std\n",
    "        y = (X[i+lenght] - mean)/std\n",
    "        squared_error += np.square(x[-1]-y)\n",
    "    return squared_error/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5d3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generated data\n",
    "def mean_squared_error(dataset):\n",
    "    mse=0\n",
    "    for X_batch, y_batch in dataset:\n",
    "        mse += np.mean(np.square(X_batch[:, -1, 3:4]-y_batch[:, 3:4]))\n",
    "    mse /= len(dataset)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82bae3",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63452d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3]))\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def mape(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])/y_true[:,3]))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3])))\n",
    "def ar(y_true, y_pred):\n",
    "    mask = tf.cast(y_pred[1:,3] > y_true[:-1,3],tf.float32)\n",
    "    return tf.reduce_mean((y_true[1:,3]-y_true[:-1,3])*mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d7d3",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0763cd9",
   "metadata": {},
   "source": [
    "## Perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e3849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(x, y, fake_output):\n",
    "    a1=0.01\n",
    "    g_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_mse = tf.keras.losses.MSE(x, y)\n",
    "    return a1*g_mse + (1-a1)*g_loss, g_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda87fb",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a10957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(n_sequence, n_features):\n",
    "    inputs = Input(shape=(n_sequence, n_features,))\n",
    "    \n",
    "    # Define the first Densely Connected Bidirectional LSTM layer\n",
    "    lstm_1 = Bidirectional(LSTM(units=10, return_sequences=True, activation=None, kernel_initializer='random_normal', dropout=0.3))(inputs)\n",
    "    lstm_1_batch_norm = BatchNormalization()(lstm_1)\n",
    "    lstm_1_LRelu = LeakyReLU(alpha=0.3)(lstm_1_batch_norm)\n",
    "    lstm_1_dropout = Dropout(0.3)(lstm_1_LRelu)\n",
    "    \n",
    "    # Define the second Densely Connected Bidirectional LSTM layer\n",
    "    lstm_2_input = concatenate([lstm_1, inputs], axis=2)\n",
    "    lstm_2 = Bidirectional(LSTM(units=10, return_sequences=False, activation=None, kernel_initializer='random_normal', dropout=0.3))(lstm_2_input)\n",
    "    lstm_2_batch_norm = BatchNormalization()(lstm_2)\n",
    "    lstm_2_LRelu = LeakyReLU(alpha=0.3)(lstm_2_batch_norm)\n",
    "    lstm_2_dropout = Dropout(0.3)(lstm_2_LRelu)\n",
    "    \n",
    "    # Define the output layer\n",
    "    output_dense = Dense(n_features, activation=None)(lstm_2_dropout)\n",
    "    output = LeakyReLU(alpha=0.3)(output_dense)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(loss=None, metrics=[mse, mae, mape, rmse, ar])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73434c04",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbabdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=72, input_shape=((n_sequence+1) * n_features,), activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(tf.keras.layers.GaussianNoise(stddev=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=100, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=10, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1 ,activation='sigmoid'))\n",
    "    model.compile(loss=discriminator_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29f52d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5acba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_def(sequences, sequences_end):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_prediction = generator(sequences, training=True)\n",
    "\n",
    "        sequences_true = tf.concat((sequences, sequences_end[:, None, :]), axis=1)\n",
    "        sequences_fake = tf.concat((sequences, generated_prediction[:, None, :]), axis=1)\n",
    "\n",
    "        real_output = discriminator(sequences_true, training=True)\n",
    "        fake_output = discriminator(sequences_fake, training=True)\n",
    "\n",
    "        gen_loss, gen_mse_loss = generator_loss(generated_prediction, \n",
    "                                                sequences_end, \n",
    "                                                fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)\n",
    "\n",
    "def test_step_def(sequences, sequences_end):\n",
    "    generated_prediction = generator(sequences, training=False)\n",
    "\n",
    "    sequences_true = tf.concat((sequences, sequences_end[:,None,:]), axis=1)\n",
    "    sequences_fake = tf.concat((sequences, generated_prediction[:,None,:]), axis=1)\n",
    "\n",
    "    real_output = discriminator(sequences_true, training=False)\n",
    "    fake_output = discriminator(sequences_fake, training=False)\n",
    "\n",
    "    gen_loss, gen_mse_loss = generator_loss(generated_prediction, sequences_end, fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81363ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, dataset_val, epochs):\n",
    "    history = np.empty(shape = (8, epochs))\n",
    "    history_val = np.empty(shape = (8, epochs))\n",
    "    len_dataset = len(dataset)\n",
    "    len_dataset_val = len(dataset_val)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        cur_dis_loss = 0\n",
    "        cur_gen_loss = 0\n",
    "        cur_gen_mse_loss = 0\n",
    "        for sequence_batch, sequence_end_batch in dataset:\n",
    "            aux_cur_losses = train_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                      tf.cast(sequence_end_batch, tf.float32))\n",
    "            cur_gen_loss += aux_cur_losses[0]/len_dataset\n",
    "            cur_dis_loss += aux_cur_losses[1]/len_dataset\n",
    "            cur_gen_mse_loss += aux_cur_losses[2]/len_dataset\n",
    "        cur_gen_metrics = generator.evaluate(dataset,verbose=False)[1:]\n",
    "\n",
    "        history[:, epoch] = cur_gen_loss, cur_dis_loss, cur_gen_mse_loss, *cur_gen_metrics\n",
    "\n",
    "        cur_gen_metrics_val = generator.evaluate(dataset_val,verbose=False)[1: ]\n",
    "\n",
    "        cur_gen_loss_val = 0\n",
    "        cur_dis_loss_val = 0\n",
    "        cur_gen_mse_loss_val = 0\n",
    "        for sequence_batch, sequence_end_batch in dataset_val:\n",
    "            aux_cur_losses_val = test_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                         tf.cast(sequence_end_batch, tf.float32))\n",
    "            cur_gen_loss_val += aux_cur_losses_val[0]/len_dataset_val\n",
    "            cur_dis_loss_val += aux_cur_losses_val[1]/len_dataset_val\n",
    "            cur_gen_mse_loss_val += aux_cur_losses_val[2]/len_dataset_val\n",
    "    \n",
    "\n",
    "\n",
    "        history_val[:, epoch] = cur_gen_loss_val, cur_dis_loss_val, cur_gen_mse_loss_val, *cur_gen_metrics_val\n",
    "\n",
    "        print ('Time for epoch {} is {} sec Generator Loss: {},  Discriminator_loss: {}'\n",
    "               .format(epoch + 1, time.time()-start, cur_gen_loss, cur_dis_loss))\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "#         if(cur_gen_loss > 0.85):\n",
    "#                 break;\n",
    "    return history, history_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca9168",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5119da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history_val):\n",
    "    metrics = [\"gen_loss\",\"dis_loss\",\"gen_mse_loss\", 'mse','mae','mape','rmse','ar']\n",
    "    for i, metric_name in enumerate(metrics):  \n",
    "        plt.figure()\n",
    "        plt.title(metric_name)\n",
    "        plt.plot(history[i], label='train')\n",
    "        plt.plot(history_val[i], label='test')\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aae39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(sequence, target, model):\n",
    "    y_pred = model.predict(sequence)[...,3]\n",
    "    y_true = target[...,3]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"closing price\")\n",
    "    plt.plot(y_true, label=\"true\")\n",
    "    plt.plot(y_pred, label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6a41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results(history):\n",
    "    min_index = np.argmin(history[3, :])\n",
    "    return history[:, min_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc15b2",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef9e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "n_sequence = window\n",
    "n_features = 7\n",
    "n_batch = 50\n",
    "EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb9e4a",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13605d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>112.093689</td>\n",
       "      <td>114.125237</td>\n",
       "      <td>110.301147</td>\n",
       "      <td>112.810707</td>\n",
       "      <td>64.370781</td>\n",
       "      <td>8933363</td>\n",
       "      <td>109.285373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-01-11</td>\n",
       "      <td>112.691208</td>\n",
       "      <td>115.798279</td>\n",
       "      <td>111.496178</td>\n",
       "      <td>113.766731</td>\n",
       "      <td>64.916260</td>\n",
       "      <td>8235472</td>\n",
       "      <td>109.667783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-01-12</td>\n",
       "      <td>114.364243</td>\n",
       "      <td>116.634796</td>\n",
       "      <td>113.049713</td>\n",
       "      <td>114.244743</td>\n",
       "      <td>65.189064</td>\n",
       "      <td>7116775</td>\n",
       "      <td>110.994264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-01-13</td>\n",
       "      <td>114.663002</td>\n",
       "      <td>115.678780</td>\n",
       "      <td>110.659653</td>\n",
       "      <td>113.049713</td>\n",
       "      <td>64.507133</td>\n",
       "      <td>8880226</td>\n",
       "      <td>111.663481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-01-14</td>\n",
       "      <td>115.619026</td>\n",
       "      <td>117.889580</td>\n",
       "      <td>112.332695</td>\n",
       "      <td>114.364243</td>\n",
       "      <td>65.257248</td>\n",
       "      <td>11460604</td>\n",
       "      <td>112.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>118.432121</td>\n",
       "      <td>119.703636</td>\n",
       "      <td>118.298279</td>\n",
       "      <td>118.451241</td>\n",
       "      <td>106.144394</td>\n",
       "      <td>2817819</td>\n",
       "      <td>119.302103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>119.502869</td>\n",
       "      <td>119.598473</td>\n",
       "      <td>118.747612</td>\n",
       "      <td>119.206497</td>\n",
       "      <td>106.821190</td>\n",
       "      <td>1842111</td>\n",
       "      <td>118.986615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>119.598473</td>\n",
       "      <td>121.032501</td>\n",
       "      <td>118.986618</td>\n",
       "      <td>119.330788</td>\n",
       "      <td>106.932564</td>\n",
       "      <td>3781499</td>\n",
       "      <td>118.822179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>119.837479</td>\n",
       "      <td>119.961761</td>\n",
       "      <td>117.820267</td>\n",
       "      <td>118.355644</td>\n",
       "      <td>106.058739</td>\n",
       "      <td>3647402</td>\n",
       "      <td>118.625238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>118.355644</td>\n",
       "      <td>119.359467</td>\n",
       "      <td>118.193115</td>\n",
       "      <td>118.871895</td>\n",
       "      <td>106.521347</td>\n",
       "      <td>3535794</td>\n",
       "      <td>118.703633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5278 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "5    2000-01-10  112.093689  114.125237  110.301147  112.810707   64.370781   \n",
       "6    2000-01-11  112.691208  115.798279  111.496178  113.766731   64.916260   \n",
       "7    2000-01-12  114.364243  116.634796  113.049713  114.244743   65.189064   \n",
       "8    2000-01-13  114.663002  115.678780  110.659653  113.049713   64.507133   \n",
       "9    2000-01-14  115.619026  117.889580  112.332695  114.364243   65.257248   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "5278 2020-12-23  118.432121  119.703636  118.298279  118.451241  106.144394   \n",
       "5279 2020-12-24  119.502869  119.598473  118.747612  119.206497  106.821190   \n",
       "5280 2020-12-28  119.598473  121.032501  118.986618  119.330788  106.932564   \n",
       "5281 2020-12-29  119.837479  119.961761  117.820267  118.355644  106.058739   \n",
       "5282 2020-12-30  118.355644  119.359467  118.193115  118.871895  106.521347   \n",
       "\n",
       "        Volume          Ma  \n",
       "5      8933363  109.285373  \n",
       "6      8235472  109.667783  \n",
       "7      7116775  110.994264  \n",
       "8      8880226  111.663481  \n",
       "9     11460604  112.476100  \n",
       "...        ...         ...  \n",
       "5278   2817819  119.302103  \n",
       "5279   1842111  118.986615  \n",
       "5280   3781499  118.822179  \n",
       "5281   3647402  118.625238  \n",
       "5282   3535794  118.703633  \n",
       "\n",
       "[5278 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_code = \"IBM\"\n",
    "start = dt.datetime(2000, 1, 1)\n",
    "end = dt.datetime(2020, 12, 31)\n",
    "raw_data = pdr.get_data_yahoo(stock_code, start, end,threads=False, proxy=\"http://127.0.0.1:7890\")\n",
    "df = raw_data.dropna();\n",
    "df = df.reset_index(level=0)\n",
    "df = add_Ma(df, window)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 7)]       0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 5, 20)        1440        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 5, 27)        0           ['bidirectional[0][0]',          \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 20)          3040        ['concatenate[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 20)          80          ['bidirectional_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 20)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20)           0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 7)            147         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 7)            0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,707\n",
      "Trainable params: 4,667\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n",
      "Time for epoch 1 is 15.783736944198608 sec Generator Loss: 0.7359350323677063,  Discriminator_loss: 1.3695809841156006\n",
      "Time for epoch 2 is 1.2448630332946777 sec Generator Loss: 0.7461550831794739,  Discriminator_loss: 1.3137840032577515\n",
      "Time for epoch 3 is 1.0689852237701416 sec Generator Loss: 0.76786869764328,  Discriminator_loss: 1.225453495979309\n",
      "Time for epoch 4 is 1.0586879253387451 sec Generator Loss: 0.8411397337913513,  Discriminator_loss: 1.1202538013458252\n",
      "Time for epoch 5 is 1.0265047550201416 sec Generator Loss: 0.9391834139823914,  Discriminator_loss: 1.0497466325759888\n",
      "Time for epoch 6 is 1.0095922946929932 sec Generator Loss: 1.0156339406967163,  Discriminator_loss: 1.0361727476119995\n",
      "Time for epoch 7 is 1.0138983726501465 sec Generator Loss: 1.0704058408737183,  Discriminator_loss: 1.0308014154434204\n",
      "Time for epoch 8 is 1.0209856033325195 sec Generator Loss: 1.0971298217773438,  Discriminator_loss: 1.041593313217163\n",
      "Time for epoch 9 is 1.1126708984375 sec Generator Loss: 1.122986912727356,  Discriminator_loss: 1.0643988847732544\n",
      "Time for epoch 10 is 1.0502443313598633 sec Generator Loss: 1.1164917945861816,  Discriminator_loss: 1.0977414846420288\n",
      "Time for epoch 11 is 1.037790060043335 sec Generator Loss: 1.1563925743103027,  Discriminator_loss: 1.1067012548446655\n",
      "Time for epoch 12 is 1.0209989547729492 sec Generator Loss: 1.110538125038147,  Discriminator_loss: 1.1466710567474365\n",
      "Time for epoch 13 is 1.0367004871368408 sec Generator Loss: 1.1066780090332031,  Discriminator_loss: 1.138421654701233\n",
      "Time for epoch 14 is 1.0320446491241455 sec Generator Loss: 1.081784963607788,  Discriminator_loss: 1.1491599082946777\n",
      "Time for epoch 15 is 1.034911870956421 sec Generator Loss: 1.0440815687179565,  Discriminator_loss: 1.167375087738037\n",
      "Time for epoch 16 is 1.150542974472046 sec Generator Loss: 1.017987847328186,  Discriminator_loss: 1.1811857223510742\n",
      "Time for epoch 17 is 1.0331518650054932 sec Generator Loss: 1.0171383619308472,  Discriminator_loss: 1.194966197013855\n",
      "Time for epoch 18 is 1.0412797927856445 sec Generator Loss: 0.9982414245605469,  Discriminator_loss: 1.2017037868499756\n",
      "Time for epoch 19 is 1.0392100811004639 sec Generator Loss: 0.9710512161254883,  Discriminator_loss: 1.2200120687484741\n",
      "Time for epoch 20 is 1.0599901676177979 sec Generator Loss: 0.9678214192390442,  Discriminator_loss: 1.2194265127182007\n",
      "Time for epoch 21 is 1.0477588176727295 sec Generator Loss: 0.9496666789054871,  Discriminator_loss: 1.226367712020874\n",
      "Time for epoch 22 is 0.984588623046875 sec Generator Loss: 0.9684038162231445,  Discriminator_loss: 1.2189781665802002\n",
      "Time for epoch 23 is 0.9754300117492676 sec Generator Loss: 0.9498185515403748,  Discriminator_loss: 1.2298921346664429\n",
      "Time for epoch 24 is 0.9698402881622314 sec Generator Loss: 0.9515775442123413,  Discriminator_loss: 1.2279880046844482\n",
      "Time for epoch 25 is 0.9661920070648193 sec Generator Loss: 0.9345310926437378,  Discriminator_loss: 1.2370187044143677\n",
      "Time for epoch 26 is 1.0757193565368652 sec Generator Loss: 0.9520630836486816,  Discriminator_loss: 1.2277017831802368\n",
      "Time for epoch 27 is 1.4490158557891846 sec Generator Loss: 0.9331192970275879,  Discriminator_loss: 1.237329363822937\n",
      "Time for epoch 28 is 1.1837234497070312 sec Generator Loss: 0.9136882424354553,  Discriminator_loss: 1.274309515953064\n",
      "Time for epoch 29 is 1.1935198307037354 sec Generator Loss: 0.8823986053466797,  Discriminator_loss: 1.3175454139709473\n",
      "Time for epoch 30 is 1.1694755554199219 sec Generator Loss: 0.8729804754257202,  Discriminator_loss: 1.3097115755081177\n",
      "Time for epoch 31 is 1.2495617866516113 sec Generator Loss: 0.862970232963562,  Discriminator_loss: 1.3458833694458008\n",
      "Time for epoch 32 is 1.2364442348480225 sec Generator Loss: 0.8693316578865051,  Discriminator_loss: 1.3307281732559204\n",
      "Time for epoch 33 is 1.167229413986206 sec Generator Loss: 0.8637659549713135,  Discriminator_loss: 1.3116461038589478\n",
      "Time for epoch 34 is 1.2386853694915771 sec Generator Loss: 0.8475862741470337,  Discriminator_loss: 1.3208714723587036\n",
      "Time for epoch 35 is 1.1885037422180176 sec Generator Loss: 0.8633178472518921,  Discriminator_loss: 1.3326627016067505\n",
      "Time for epoch 36 is 1.2062842845916748 sec Generator Loss: 0.8506855368614197,  Discriminator_loss: 1.338880181312561\n",
      "Time for epoch 37 is 1.1511976718902588 sec Generator Loss: 0.8419193029403687,  Discriminator_loss: 1.3282861709594727\n",
      "Time for epoch 38 is 1.1667578220367432 sec Generator Loss: 0.8346410989761353,  Discriminator_loss: 1.3282897472381592\n",
      "Time for epoch 39 is 1.161355972290039 sec Generator Loss: 0.8426216244697571,  Discriminator_loss: 1.3302698135375977\n",
      "Time for epoch 40 is 1.174102544784546 sec Generator Loss: 0.8381426930427551,  Discriminator_loss: 1.3217893838882446\n",
      "Time for epoch 41 is 1.155714750289917 sec Generator Loss: 0.8282769918441772,  Discriminator_loss: 1.3329358100891113\n",
      "Time for epoch 42 is 1.153527021408081 sec Generator Loss: 0.829553484916687,  Discriminator_loss: 1.3310450315475464\n",
      "Time for epoch 43 is 1.1520168781280518 sec Generator Loss: 0.8232529163360596,  Discriminator_loss: 1.329211950302124\n",
      "Time for epoch 44 is 1.129915475845337 sec Generator Loss: 0.8201854825019836,  Discriminator_loss: 1.323462963104248\n",
      "Time for epoch 45 is 1.1422457695007324 sec Generator Loss: 0.8200649619102478,  Discriminator_loss: 1.3237574100494385\n",
      "Time for epoch 46 is 1.0397560596466064 sec Generator Loss: 0.8168953061103821,  Discriminator_loss: 1.331870675086975\n",
      "Time for epoch 47 is 1.0778281688690186 sec Generator Loss: 0.8123047351837158,  Discriminator_loss: 1.3304674625396729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 48 is 1.0194733142852783 sec Generator Loss: 0.8213186860084534,  Discriminator_loss: 1.320358395576477\n",
      "Time for epoch 49 is 1.04707932472229 sec Generator Loss: 0.8319101333618164,  Discriminator_loss: 1.3295073509216309\n",
      "Time for epoch 50 is 1.0052082538604736 sec Generator Loss: 0.8239537477493286,  Discriminator_loss: 1.325439214706421\n",
      "Time for epoch 51 is 0.8918671607971191 sec Generator Loss: 0.8259411454200745,  Discriminator_loss: 1.3329906463623047\n",
      "Time for epoch 52 is 0.8051657676696777 sec Generator Loss: 0.8264748454093933,  Discriminator_loss: 1.331255316734314\n",
      "Time for epoch 53 is 0.8836319446563721 sec Generator Loss: 0.8268818259239197,  Discriminator_loss: 1.339138150215149\n",
      "Time for epoch 54 is 0.8628244400024414 sec Generator Loss: 0.8447136878967285,  Discriminator_loss: 1.3307714462280273\n",
      "Time for epoch 55 is 0.9079728126525879 sec Generator Loss: 0.8264814019203186,  Discriminator_loss: 1.3350446224212646\n",
      "Time for epoch 56 is 0.8219590187072754 sec Generator Loss: 0.8160235285758972,  Discriminator_loss: 1.33921217918396\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "df = add_Ma(df)\n",
    "data_gen_train, data_gen_test = get_gen_train_test(df, n_sequence, n_batch)\n",
    "\n",
    "generator = make_generator_model(n_sequence, n_features)\n",
    "discriminator=make_discriminator_model(n_features)\n",
    "\n",
    "learning_rate=1e-4\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(sequences, sequences_end):\n",
    "  return train_step_def(sequences, sequences_end)\n",
    "\n",
    "@tf.function\n",
    "def test_step(sequences, sequences_end):\n",
    "  return test_step_def(sequences, sequences_end)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'+stock_code\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "history, history_val = train(data_gen_train, data_gen_test, EPOCHS)\n",
    "\n",
    "plot_history(history, history_val)\n",
    "plot_frame(*data_gen_test[0], generator)\n",
    "\n",
    "print(\"[MSE Baseline] train:\",mean_squared_error(data_gen_train),\" test:\", mean_squared_error(data_gen_test))\n",
    "now = datetime.datetime.now()\n",
    "delta = now - start_time\n",
    "print(\"Delta time:\", delta)\n",
    "generator.save(\"dc_bi_lstm_ffnn.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
