{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0627faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab6eea",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cfe3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU, BatchNormalization, Bidirectional, LSTM, concatenate, Flatten\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "from pandas_datareader import data as pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ea8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframe(df):\n",
    "  for column_name in df.columns:\n",
    "    plt.figure()\n",
    "    plt.title(column_name)\n",
    "    plt.plot(df[column_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64236b5a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94817d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_Ma(df, window=5):\n",
    "    for i in range(window, df.shape[0]):\n",
    "        sum = 0.0\n",
    "        for k in range(1, window+1):\n",
    "            sum += df.iloc[i-k, 4]\n",
    "        df.loc[df.index[i], 'Ma'] = np.round(sum/window, 6)\n",
    "    return df[window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68bdddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standarized_TimeseriesGenerator(tf.keras.preprocessing.sequence.TimeseriesGenerator):\n",
    "  def __getitem__(self, index):\n",
    "    samples, targets  = super(Standarized_TimeseriesGenerator, self).__getitem__(index)\n",
    "    mean = samples.mean(axis=1)\n",
    "    std = samples.std(axis=1)\n",
    "    samples = (samples - mean[:,None,:])/std[:,None,:]\n",
    "    targets = (targets - mean)/std\n",
    "    return samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20efe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_train_test(dataframe, n_sequence, n_batch):\n",
    "    data = dataframe.drop(columns='Date').to_numpy()\n",
    "    targets = data\n",
    "    n_samples = data.shape[0]\n",
    "    train_test_split=int(n_samples*0.9)\n",
    "\n",
    "    data_gen_train = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = 0,\n",
    "                                end_index = train_test_split,\n",
    "                                shuffle = True)\n",
    "    data_gen_test = Standarized_TimeseriesGenerator(data, targets,\n",
    "                                length=n_sequence, sampling_rate=1,\n",
    "                                stride=1, batch_size=n_batch,\n",
    "                                start_index = train_test_split,\n",
    "                                end_index = n_samples-1)\n",
    "\n",
    "    return data_gen_train, data_gen_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1596c86e",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36e1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(X, lenght = 5):\n",
    "    squared_error = 0\n",
    "    for i in range(0, X.shape[0] - lenght):\n",
    "        x = X[i:i+lenght]\n",
    "        mean = x.mean()\n",
    "        std = x.std()\n",
    "        x = (x - mean)/std\n",
    "        y = (X[i+lenght] - mean)/std\n",
    "        squared_error += np.square(x[-1]-y)\n",
    "    return squared_error/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5d3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generated data\n",
    "def mean_squared_error(dataset):\n",
    "    mse=0\n",
    "    for X_batch, y_batch in dataset:\n",
    "        mse += np.mean(np.square(X_batch[:, -1, 3:4]-y_batch[:, 3:4]))\n",
    "    mse /= len(dataset)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82bae3",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63452d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3]))\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def mape(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.backend.abs((y_true[:,3]-y_pred[:,3])/y_true[:,3]))\n",
    "    # ***The absolute is over the whole thing as y_true can be negative\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true[:,3]-y_pred[:,3])))\n",
    "def ar(y_true, y_pred):\n",
    "    mask = tf.cast(y_pred[1:,3] > y_true[:-1,3],tf.float32)\n",
    "    return tf.reduce_mean((y_true[1:,3]-y_true[:-1,3])*mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35d7d3",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0763cd9",
   "metadata": {},
   "source": [
    "## Perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e3849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(x, y, fake_output):\n",
    "    a1=0.01\n",
    "    g_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_mse = tf.keras.losses.MSE(x, y)\n",
    "    return a1*g_mse + (1-a1)*g_loss, g_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda87fb",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a10957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(n_sequence, n_features):\n",
    "    inputs = Input(shape=(n_sequence, n_features,))\n",
    "    \n",
    "    # Define the first Densely Connected Bidirectional LSTM layer\n",
    "    lstm_1 = Bidirectional(LSTM(units=10, return_sequences=True, activation=None, kernel_initializer='random_normal', dropout=0.3))(inputs)\n",
    "    lstm_1_batch_norm = BatchNormalization()(lstm_1)\n",
    "    lstm_1_LRelu = LeakyReLU(alpha=0.3)(lstm_1_batch_norm)\n",
    "    lstm_1_dropout = Dropout(0.3)(lstm_1_LRelu)\n",
    "    \n",
    "    # Define the second Densely Connected Bidirectional LSTM layer\n",
    "    lstm_2_input = concatenate([lstm_1, inputs], axis=2)\n",
    "    lstm_2 = Bidirectional(LSTM(units=10, return_sequences=False, activation=None, kernel_initializer='random_normal', dropout=0.3))(lstm_2_input)\n",
    "    lstm_2_batch_norm = BatchNormalization()(lstm_2)\n",
    "    lstm_2_LRelu = LeakyReLU(alpha=0.3)(lstm_2_batch_norm)\n",
    "    lstm_2_dropout = Dropout(0.3)(lstm_2_LRelu)\n",
    "    \n",
    "    # Define the output layer\n",
    "    output_dense = Dense(n_features, activation=None)(lstm_2_dropout)\n",
    "    output = LeakyReLU(alpha=0.3)(output_dense)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(loss=None, metrics=[mse, mae, mape, rmse, ar])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73434c04",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbabdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(n_features):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=72, input_shape=((n_sequence+1) * n_features,), activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(tf.keras.layers.GaussianNoise(stddev=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=100, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=10, activation=None, kernel_initializer='random_normal'))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1 ,activation='sigmoid'))\n",
    "    model.compile(loss=discriminator_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29f52d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5acba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_def(sequences, sequences_end):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_prediction = generator(sequences, training=True)\n",
    "\n",
    "        sequences_true = tf.concat((sequences, sequences_end[:, None, :]), axis=1)\n",
    "        sequences_fake = tf.concat((sequences, generated_prediction[:, None, :]), axis=1)\n",
    "\n",
    "        real_output = discriminator(sequences_true, training=True)\n",
    "        fake_output = discriminator(sequences_fake, training=True)\n",
    "\n",
    "        gen_loss, gen_mse_loss = generator_loss(generated_prediction, \n",
    "                                                sequences_end, \n",
    "                                                fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)\n",
    "\n",
    "def test_step_def(sequences, sequences_end):\n",
    "    generated_prediction = generator(sequences, training=False)\n",
    "\n",
    "    sequences_true = tf.concat((sequences, sequences_end[:,None,:]), axis=1)\n",
    "    sequences_fake = tf.concat((sequences, generated_prediction[:,None,:]), axis=1)\n",
    "\n",
    "    real_output = discriminator(sequences_true, training=False)\n",
    "    fake_output = discriminator(sequences_fake, training=False)\n",
    "\n",
    "    gen_loss, gen_mse_loss = generator_loss(generated_prediction, sequences_end, fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    return tf.reduce_mean(gen_loss), tf.reduce_mean(disc_loss), tf.reduce_mean(gen_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81363ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, dataset_val, epochs):\n",
    "    history = np.empty(shape = (8, epochs))\n",
    "    history_val = np.empty(shape = (8, epochs))\n",
    "    len_dataset = len(dataset)\n",
    "    len_dataset_val = len(dataset_val)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        cur_dis_loss = 0\n",
    "        cur_gen_loss = 0\n",
    "        cur_gen_mse_loss = 0\n",
    "        for sequence_batch, sequence_end_batch in dataset:\n",
    "            aux_cur_losses = train_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                      tf.cast(sequence_end_batch, tf.float32))\n",
    "            cur_gen_loss += aux_cur_losses[0]/len_dataset\n",
    "            cur_dis_loss += aux_cur_losses[1]/len_dataset\n",
    "            cur_gen_mse_loss += aux_cur_losses[2]/len_dataset\n",
    "        cur_gen_metrics = generator.evaluate(dataset,verbose=False)[1:]\n",
    "\n",
    "        history[:, epoch] = cur_gen_loss, cur_dis_loss, cur_gen_mse_loss, *cur_gen_metrics\n",
    "\n",
    "        cur_gen_metrics_val = generator.evaluate(dataset_val,verbose=False)[1: ]\n",
    "\n",
    "        cur_gen_loss_val = 0\n",
    "        cur_dis_loss_val = 0\n",
    "        cur_gen_mse_loss_val = 0\n",
    "        for sequence_batch, sequence_end_batch in dataset_val:\n",
    "            aux_cur_losses_val = test_step(tf.cast(sequence_batch, tf.float32), \n",
    "                                         tf.cast(sequence_end_batch, tf.float32))\n",
    "            cur_gen_loss_val += aux_cur_losses_val[0]/len_dataset_val\n",
    "            cur_dis_loss_val += aux_cur_losses_val[1]/len_dataset_val\n",
    "            cur_gen_mse_loss_val += aux_cur_losses_val[2]/len_dataset_val\n",
    "    \n",
    "\n",
    "\n",
    "        history_val[:, epoch] = cur_gen_loss_val, cur_dis_loss_val, cur_gen_mse_loss_val, *cur_gen_metrics_val\n",
    "\n",
    "        print ('Time for epoch {} is {} sec Generator Loss: {},  Discriminator_loss: {}'\n",
    "               .format(epoch + 1, time.time()-start, cur_gen_loss, cur_dis_loss))\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "#         if(cur_gen_loss > 0.85):\n",
    "#                 break;\n",
    "    return history, history_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca9168",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5119da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, history_val):\n",
    "    metrics = [\"gen_loss\",\"dis_loss\",\"gen_mse_loss\", 'mse','mae','mape','rmse','ar']\n",
    "    for i, metric_name in enumerate(metrics):  \n",
    "        plt.figure()\n",
    "        plt.title(metric_name)\n",
    "        plt.plot(history[i], label='train')\n",
    "        plt.plot(history_val[i], label='test')\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aae39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(sequence, target, model):\n",
    "    y_pred = model.predict(sequence)[...,3]\n",
    "    y_true = target[...,3]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"closing price\")\n",
    "    plt.plot(y_true, label=\"true\")\n",
    "    plt.plot(y_pred, label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e6a41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_results(history):\n",
    "    min_index = np.argmin(history[3, :])\n",
    "    return history[:, min_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc15b2",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ef9e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "n_sequence = window\n",
    "n_features = 7\n",
    "n_batch = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb9e4a",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13605d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.912946</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>0.872768</td>\n",
       "      <td>0.742829</td>\n",
       "      <td>505064000</td>\n",
       "      <td>0.915960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-01-11</td>\n",
       "      <td>0.856585</td>\n",
       "      <td>0.887277</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.704832</td>\n",
       "      <td>441548800</td>\n",
       "      <td>0.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-01-12</td>\n",
       "      <td>0.848214</td>\n",
       "      <td>0.852679</td>\n",
       "      <td>0.772321</td>\n",
       "      <td>0.778460</td>\n",
       "      <td>0.662561</td>\n",
       "      <td>976068800</td>\n",
       "      <td>0.873214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-01-13</td>\n",
       "      <td>0.843610</td>\n",
       "      <td>0.881696</td>\n",
       "      <td>0.825893</td>\n",
       "      <td>0.863839</td>\n",
       "      <td>0.735229</td>\n",
       "      <td>1032684800</td>\n",
       "      <td>0.843192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-01-14</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.912946</td>\n",
       "      <td>0.887277</td>\n",
       "      <td>0.896763</td>\n",
       "      <td>0.763251</td>\n",
       "      <td>390376000</td>\n",
       "      <td>0.846317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>132.160004</td>\n",
       "      <td>132.429993</td>\n",
       "      <td>130.779999</td>\n",
       "      <td>130.960007</td>\n",
       "      <td>129.209290</td>\n",
       "      <td>88223700</td>\n",
       "      <td>128.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5279</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>131.320007</td>\n",
       "      <td>133.460007</td>\n",
       "      <td>131.100006</td>\n",
       "      <td>131.970001</td>\n",
       "      <td>130.205765</td>\n",
       "      <td>54930100</td>\n",
       "      <td>129.286002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>133.990005</td>\n",
       "      <td>137.339996</td>\n",
       "      <td>133.509995</td>\n",
       "      <td>136.690002</td>\n",
       "      <td>134.862671</td>\n",
       "      <td>124486200</td>\n",
       "      <td>129.940002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>138.050003</td>\n",
       "      <td>138.789993</td>\n",
       "      <td>134.339996</td>\n",
       "      <td>134.869995</td>\n",
       "      <td>133.067017</td>\n",
       "      <td>121047300</td>\n",
       "      <td>131.946002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>135.580002</td>\n",
       "      <td>135.990005</td>\n",
       "      <td>133.399994</td>\n",
       "      <td>133.720001</td>\n",
       "      <td>131.932373</td>\n",
       "      <td>96452100</td>\n",
       "      <td>133.274002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5278 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "5    2000-01-10    0.910714    0.912946    0.845982    0.872768    0.742829   \n",
       "6    2000-01-11    0.856585    0.887277    0.808036    0.828125    0.704832   \n",
       "7    2000-01-12    0.848214    0.852679    0.772321    0.778460    0.662561   \n",
       "8    2000-01-13    0.843610    0.881696    0.825893    0.863839    0.735229   \n",
       "9    2000-01-14    0.892857    0.912946    0.887277    0.896763    0.763251   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "5278 2020-12-23  132.160004  132.429993  130.779999  130.960007  129.209290   \n",
       "5279 2020-12-24  131.320007  133.460007  131.100006  131.970001  130.205765   \n",
       "5280 2020-12-28  133.990005  137.339996  133.509995  136.690002  134.862671   \n",
       "5281 2020-12-29  138.050003  138.789993  134.339996  134.869995  133.067017   \n",
       "5282 2020-12-30  135.580002  135.990005  133.399994  133.720001  131.932373   \n",
       "\n",
       "          Volume          Ma  \n",
       "5      505064000    0.915960  \n",
       "6      441548800    0.890625  \n",
       "7      976068800    0.873214  \n",
       "8     1032684800    0.843192  \n",
       "9      390376000    0.846317  \n",
       "...          ...         ...  \n",
       "5278    88223700  128.656000  \n",
       "5279    54930100  129.286002  \n",
       "5280   124486200  129.940002  \n",
       "5281   121047300  131.946002  \n",
       "5282    96452100  133.274002  \n",
       "\n",
       "[5278 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_code = \"AAPL\"\n",
    "start = dt.datetime(2000, 1, 1)\n",
    "end = dt.datetime(2020, 12, 31)\n",
    "raw_data = pdr.get_data_yahoo(stock_code, start, end,threads=False, proxy=\"http://127.0.0.1:7890\")\n",
    "df = raw_data.dropna();\n",
    "df = df.reset_index(level=0)\n",
    "df = add_Ma(df, window)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 7)]       0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 5, 20)        1440        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 5, 27)        0           ['bidirectional[0][0]',          \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 20)          3040        ['concatenate[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 20)          80          ['bidirectional_1[0][0]']        \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 20)           0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20)           0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 7)            147         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 7)            0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,707\n",
      "Trainable params: 4,667\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n",
      "Time for epoch 1 is 16.033422231674194 sec Generator Loss: 0.7403981685638428,  Discriminator_loss: 1.3696149587631226\n",
      "Time for epoch 2 is 0.9489197731018066 sec Generator Loss: 0.7333282828330994,  Discriminator_loss: 1.3276183605194092\n",
      "Time for epoch 3 is 0.9038128852844238 sec Generator Loss: 0.7525513172149658,  Discriminator_loss: 1.281117558479309\n",
      "Time for epoch 4 is 0.9219470024108887 sec Generator Loss: 0.8054788112640381,  Discriminator_loss: 1.2477887868881226\n",
      "Time for epoch 5 is 1.2109742164611816 sec Generator Loss: 0.8745585680007935,  Discriminator_loss: 1.2018656730651855\n",
      "Time for epoch 6 is 1.059433937072754 sec Generator Loss: 0.9152888059616089,  Discriminator_loss: 1.1781044006347656\n",
      "Time for epoch 7 is 1.0053040981292725 sec Generator Loss: 0.9694282412528992,  Discriminator_loss: 1.1620409488677979\n",
      "Time for epoch 8 is 0.9438230991363525 sec Generator Loss: 0.9807890057563782,  Discriminator_loss: 1.1656941175460815\n",
      "Time for epoch 9 is 0.9538114070892334 sec Generator Loss: 0.993243932723999,  Discriminator_loss: 1.1709002256393433\n",
      "Time for epoch 10 is 0.9822490215301514 sec Generator Loss: 0.998164713382721,  Discriminator_loss: 1.1905686855316162\n",
      "Time for epoch 11 is 0.9446864128112793 sec Generator Loss: 1.0005695819854736,  Discriminator_loss: 1.1877081394195557\n",
      "Time for epoch 12 is 0.9403893947601318 sec Generator Loss: 1.0048781633377075,  Discriminator_loss: 1.1875485181808472\n",
      "Time for epoch 13 is 0.945465087890625 sec Generator Loss: 0.9901732206344604,  Discriminator_loss: 1.202169418334961\n",
      "Time for epoch 14 is 0.9410574436187744 sec Generator Loss: 0.9776734113693237,  Discriminator_loss: 1.213923454284668\n",
      "Time for epoch 15 is 0.9632079601287842 sec Generator Loss: 0.9710360765457153,  Discriminator_loss: 1.2290611267089844\n",
      "Time for epoch 16 is 0.9531393051147461 sec Generator Loss: 0.9472639560699463,  Discriminator_loss: 1.238835096359253\n",
      "Time for epoch 17 is 0.9999959468841553 sec Generator Loss: 0.9376228451728821,  Discriminator_loss: 1.241742730140686\n",
      "Time for epoch 18 is 0.9547703266143799 sec Generator Loss: 0.924804151058197,  Discriminator_loss: 1.2653652429580688\n",
      "Time for epoch 19 is 0.9358367919921875 sec Generator Loss: 0.9078823328018188,  Discriminator_loss: 1.2724910974502563\n",
      "Time for epoch 20 is 0.954207181930542 sec Generator Loss: 0.9099615812301636,  Discriminator_loss: 1.2667502164840698\n",
      "Time for epoch 21 is 0.9544389247894287 sec Generator Loss: 0.9148972034454346,  Discriminator_loss: 1.261683464050293\n",
      "Time for epoch 22 is 0.9408042430877686 sec Generator Loss: 0.9108938574790955,  Discriminator_loss: 1.2655627727508545\n",
      "Time for epoch 23 is 0.9407358169555664 sec Generator Loss: 0.8935410380363464,  Discriminator_loss: 1.2711347341537476\n",
      "Time for epoch 24 is 0.9339010715484619 sec Generator Loss: 0.9000019431114197,  Discriminator_loss: 1.2585265636444092\n",
      "Time for epoch 25 is 0.9668576717376709 sec Generator Loss: 0.9035482406616211,  Discriminator_loss: 1.2567157745361328\n",
      "Time for epoch 26 is 0.9620282649993896 sec Generator Loss: 0.9024972915649414,  Discriminator_loss: 1.2601041793823242\n",
      "Time for epoch 27 is 1.1793973445892334 sec Generator Loss: 0.8922737836837769,  Discriminator_loss: 1.2726808786392212\n",
      "Time for epoch 28 is 0.9352619647979736 sec Generator Loss: 0.8947734236717224,  Discriminator_loss: 1.2743643522262573\n",
      "Time for epoch 29 is 0.9326891899108887 sec Generator Loss: 0.8875036835670471,  Discriminator_loss: 1.2865242958068848\n",
      "Time for epoch 30 is 0.9495515823364258 sec Generator Loss: 0.8766450881958008,  Discriminator_loss: 1.3077805042266846\n",
      "Time for epoch 31 is 0.9456422328948975 sec Generator Loss: 0.8629780411720276,  Discriminator_loss: 1.3116509914398193\n",
      "Time for epoch 32 is 0.9602749347686768 sec Generator Loss: 0.8511857986450195,  Discriminator_loss: 1.3236163854599\n",
      "Time for epoch 33 is 0.9396014213562012 sec Generator Loss: 0.8526726961135864,  Discriminator_loss: 1.318652629852295\n",
      "Time for epoch 34 is 0.9469916820526123 sec Generator Loss: 0.8464941382408142,  Discriminator_loss: 1.328707218170166\n",
      "Time for epoch 35 is 0.9308459758758545 sec Generator Loss: 0.8391342163085938,  Discriminator_loss: 1.334214210510254\n",
      "Time for epoch 36 is 0.9749245643615723 sec Generator Loss: 0.8362452983856201,  Discriminator_loss: 1.3320235013961792\n",
      "Time for epoch 37 is 0.940521240234375 sec Generator Loss: 0.8314652442932129,  Discriminator_loss: 1.3383915424346924\n",
      "Time for epoch 38 is 0.9457211494445801 sec Generator Loss: 0.8271560668945312,  Discriminator_loss: 1.3410060405731201\n",
      "Time for epoch 39 is 0.9352667331695557 sec Generator Loss: 0.818069577217102,  Discriminator_loss: 1.346595287322998\n",
      "Time for epoch 40 is 0.9358525276184082 sec Generator Loss: 0.8217869400978088,  Discriminator_loss: 1.3512566089630127\n",
      "Time for epoch 41 is 0.9464237689971924 sec Generator Loss: 0.8164865374565125,  Discriminator_loss: 1.3480656147003174\n",
      "Time for epoch 42 is 0.9275393486022949 sec Generator Loss: 0.8127058148384094,  Discriminator_loss: 1.3479700088500977\n",
      "Time for epoch 43 is 0.924053430557251 sec Generator Loss: 0.8116627931594849,  Discriminator_loss: 1.3426811695098877\n",
      "Time for epoch 44 is 0.9470317363739014 sec Generator Loss: 0.814167857170105,  Discriminator_loss: 1.3492841720581055\n",
      "Time for epoch 45 is 0.9451389312744141 sec Generator Loss: 0.8111778497695923,  Discriminator_loss: 1.3449907302856445\n",
      "Time for epoch 46 is 1.0405230522155762 sec Generator Loss: 0.811776340007782,  Discriminator_loss: 1.3490945100784302\n",
      "Time for epoch 47 is 0.9360628128051758 sec Generator Loss: 0.8094290494918823,  Discriminator_loss: 1.345258116722107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 48 is 0.9293849468231201 sec Generator Loss: 0.8097449541091919,  Discriminator_loss: 1.348109483718872\n",
      "Time for epoch 49 is 0.930124044418335 sec Generator Loss: 0.8115081191062927,  Discriminator_loss: 1.3382664918899536\n",
      "Time for epoch 50 is 0.9249591827392578 sec Generator Loss: 0.822754442691803,  Discriminator_loss: 1.3344062566757202\n"
     ]
    }
   ],
   "source": [
    "for epochs in [200, 450, 700, 950, 1200, 1450, 1700, 1950]:\n",
    "    start_time = datetime.datetime.now()\n",
    "    df = add_Ma(df)\n",
    "    data_gen_train, data_gen_test = get_gen_train_test(df, n_sequence, n_batch)\n",
    "\n",
    "    generator = make_generator_model(n_sequence, n_features)\n",
    "    discriminator=make_discriminator_model(n_features)\n",
    "\n",
    "    learning_rate=1e-4\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(sequences, sequences_end):\n",
    "      return train_step_def(sequences, sequences_end)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(sequences, sequences_end):\n",
    "      return test_step_def(sequences, sequences_end)\n",
    "\n",
    "    checkpoint_dir = './training_checkpoints'+stock_code\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                     discriminator_optimizer=discriminator_optimizer,\n",
    "                                     generator=generator,\n",
    "                                     discriminator=discriminator)\n",
    "\n",
    "    history, history_val = train(data_gen_train, data_gen_test, epochs)\n",
    "\n",
    "    plot_history(history, history_val)\n",
    "    plot_frame(*data_gen_test[0], generator)\n",
    "\n",
    "    print(\"[MSE Baseline] train:\",mean_squared_error(data_gen_train),\" test:\", mean_squared_error(data_gen_test))\n",
    "    now = datetime.datetime.now()\n",
    "    delta = now - start_time\n",
    "    print(\"Delta time with epochs = {0}:\".format(epochs), delta)\n",
    "    generator.save(\"dcbilstm_ffnn_epochs_{0}.h5\".format(epochs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
